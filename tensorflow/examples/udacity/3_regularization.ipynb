{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/cpu:0'):  # force CPU-only usage  (bcos I have trouble w/ GPU memory)\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, \n",
    "                                         shape=(batch_size, image_size*image_size))\n",
    "        tf_train_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset  = tf.constant(test_dataset)\n",
    "        beta             = tf.placeholder(tf.float32)  # Beta for Regularization\n",
    "        \n",
    "        # Variables \n",
    "        weights  = tf.Variable(\n",
    "                    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "        biases   = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "        ## Training computation\n",
    "        # logits = X . W + b\n",
    "        logits = tf.matmul( tf_train_dataset, weights) + biases\n",
    "        # loss is avg of {cross entropy for the logits + regularized weights}\n",
    "        loss   = tf.reduce_mean( \n",
    "                    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta * tf.nn.l2_loss(weights)\n",
    "        \n",
    "        ## Optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "        \n",
    "        ## Predictions for training, validation and test \n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(\n",
    "                            tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "        test_prediction  = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "minibatch loss at step 0: 17.342541\n",
      "minibatch accuracy : 21.1%\n",
      "validation accuracy: 18.0%\n",
      "minibatch loss at step 500: 2.486070\n",
      "minibatch accuracy : 82.0%\n",
      "validation accuracy: 76.1%\n",
      "minibatch loss at step 1000: 1.770455\n",
      "minibatch accuracy : 78.1%\n",
      "validation accuracy: 77.9%\n",
      "minibatch loss at step 1500: 0.944515\n",
      "minibatch accuracy : 84.4%\n",
      "validation accuracy: 79.7%\n",
      "minibatch loss at step 2000: 0.846611\n",
      "minibatch accuracy : 86.7%\n",
      "validation accuracy: 80.7%\n",
      "minibatch loss at step 2500: 0.858664\n",
      "minibatch accuracy : 81.2%\n",
      "validation accuracy: 81.4%\n",
      "minibatch loss at step 3000: 0.770286\n",
      "minibatch accuracy : 85.2%\n",
      "validation accuracy: 81.6%\n",
      "\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "## For limiting GPU memory usage: -- Added by Atul\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a mini batch\n",
    "        batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "        # Make a dict telling the session where to find the minibatch\n",
    "        feed_dict = { tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: 1e-3 }\n",
    "        # Run the damn thing\n",
    "        _, loss_step, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        # show progress\n",
    "        if (step % 500 == 0):\n",
    "            print(\"minibatch loss at step %d: %f\" % (step, loss_step))\n",
    "            print(\"minibatch accuracy : %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont know what is an optimum value of the Regularization param, so we could run the test with different values and see which gives the best accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n",
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "params_regularization = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_values = []\n",
    "\n",
    "for R_param in params_regularization:\n",
    "    with tf.Session(graph=graph, config=config) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print (\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a mini batch\n",
    "            batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "            # Make a dict telling the session where to find the minibatch\n",
    "            feed_dict = { tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: R_param }\n",
    "            # Run the damn thing\n",
    "            _, loss_step, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            # save the TEST accuracy only\n",
    "        accuracy_values.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEOCAYAAACEiBAqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XecVPW5x/HPAyjIoq6xYgMUjSUmqBGxxTUYJWpCYkUs\nwUKMsaLGkmuJMbarBLGLYkMUFAuK3qh4WRVRwYgFgYtClqVZokIUlfrcP35nZVhmd2Z3ZufMnPm+\nX695MWdOe2Z2OM/8fs/5nWPujoiIlK9WcQcgIiLxUiIQESlzSgQiImVOiUBEpMwpEYiIlDklAhGR\nMqdEIAKYWVszW2lmm8cdS1OZ2etm1jeH9T8ysz3zHNPaZvaVmW2Wz+1Ky1AiKBHRf6r/RI8VZvZN\nymvH5rDdnA4iCVOWg2rcvau7v5nLNup/j9x9qbuv6+4f5x6htLQ2cQcg2XH3deuem9ks4BR3Hxdj\nSAVhZq3dfUWhdtciGy3se8hascYlhacWQWky6h20zKyVmV1mZjPN7FMzG2Zm60Xz2pvZI2b2uZl9\nGf16W9/MbgT2AO6JWhY3rLEjs9ZmNsrMPjazL8zsJTPbPmV+ezO72cxqo22PM7NW0byqaF8LzazG\nzPpEr6/269HMTjOzF6PndV00fzCzj4D3o9dvN7M5ZrbIzN5I7cqIYrwieu+LzOxNM9vEzO4xs7/V\nez/Pm9lpjXy2vzWzf5nZJ3Xrmtk60Xa3TdnOlma2uO4zrreP06LP6VYz+wK4KOX16Wb2bzN7JrUb\nyswONbMZ0Wc8KPUzMrNrzWxIyrI/NLNl6YKP5o2L/tafmNn9ZlaRMn+BmZ1vZlOARSmv7R19h1Jb\nnl9Hf4tNzGwjM3su+m7928yeMrNNo/XX+B5Zva42M9vAzB6O1p9pZn+q93mNNbPB0XfoQzPr2cjf\nSPJMiSA5/gQcCOwNbAksAwZF804FWgMdgQ2BM4Gl7n4BMInQuljP3f+0xlaDp4AuwGbAdOCBlHm3\nANsDuwM/AC4F3My6As8A10ev7w580Ej89btlDgV2A3aNpicAO0fbGg08Zmato3l/Bn4NHOju6wO/\nB76L4kxNOB2BfYCRjcRxGPBjoDtwrJn1dfdvgceA41OW6wuMcff/NLCd/YC3CZ/3QDM7Bjg7el+b\nApOBh1LiGgGcA2wMzI/ee2Ma68a6EtgE2IXwt/mvevOPBnpGsa3aoPvKqDtnPXdfD7gLeBH4jHCs\nuIPw3eoS7X9QtF5D36PUGO8ifAc7AQcBp9vqXZr7ARMJf9/bgHsyvH/JJ3fXo8QewL+An9d7bRaw\nV8p0F2Bx9Px0YBywc5ptvQ70bcK+NwNWAGsTuhaXAl3TLPcXYHgD21htn8BpwAvR87bASmDPRmIw\nYDGwXTRdQ0gC6Zb9CNgnen4+MKqB5er2u1/KawOAZ6LnPwM+TJn3HnBYA9s6DZhe77X/BY5NmV4r\n+uw2BvoDL9V7f5/UfUbAtcCQlPk/JCTyjH9D4BjgtZTpBcAx9ZZZAOxd77UTgRnA+g1stwcwr5G/\nad3nuXn0XVkOdEqZfzbwXMrn9V7KvA2i79h6hf6/Va4PtQiSYyvguahr4QvCr1HM7AfAUOAVYFTU\nhXO1mWXVHx51uwyMmvMLgWnRrA0JLYzWhCSULp6ZObyfufXiuCTqVvkS+IJwoNkomr1FAzEADGPV\nL/njo+ls9zubcCDD3V8BWpnZnmb2E0JC/J9GtjOn3nQn4M6Uv8+nhESwZbSP75f3cDSclyHOtMys\no5k9amZzo7/XPaz6nOrMTbNq6jb2BG4Afu3udd1HHcxsqJnNjrb7fJrtNmQzQnJL/UxmE/5udVKL\nyt9Ey3fIcvuSIyWC5JhLaCX8IHps4O4V7v6FhzM4rnD3HQm/bI8C+kTrZTpT5iRCN8L+7l4J7BC9\nboRfksuBbdOsNwfo2sA2FwPtU6bTnWL4fVxmdiChO6u3u29A6D74jlV1krkNxADwIHCkme1GOOg+\n28BydbZKeb41oZsmdVsnRI8R3nihtf7nWgv0q/f36eDukwmf4/f7jZJ06kGy/ufVsZH93gB8DewU\n/b1OZc0ieIN/86hPfxShm2d6yqyLo5h2j7Z7UL3tNvY9+pjQOtg65bWtaWayk/xTIkiOu4DrzWxL\ngKjAd1j0vKeZ7RgdYL4mHLzrDmKfANs0st11CQfdL82sA3B13Qx3X044OA6O9tfKzPaJ9jMMONTM\neketio3MbJdo1XcIB+e2ZrYD0C/De1uX8Ov5czNrC1xFaBHUGQpcY2Zdovfbra6I6+7/IrRi7gNG\nRjE35iIzW8/MOhOSz4iUecMI/et9ovfdFHcBl1lUaI+Kp4dH854GuptZr6jucT5QmbLuO8ABZra5\nmW0AXNjIftYl/I2/NrOtgfOyDdDM1gKeAO509zFptvsN8B8z24hQC0rV4PfI3ZcCTxL+Ru0tFN3P\nIXPrTApEiaA0pfv1dT2hsPe/ZrYIGM+qQusWhALrfwh922Pc/dFo3iDgd9FZJtel2e5Q4N+EX3Xv\nErqYUp1D6AKaHC33V8DcfSbQm1Co/IJQTNwpWue/CX3knwJ3suYBof77ewZ4NdrPR9F6n6XMv47w\nS7/uvd/B6oniAeBHZD54e7SddwmFy5HuPvz7me6zgP8DvnL3tzJsa/UNu48gFNafiLpW3iYU9/Fw\nrv2x0fzPCF1F7wNLotWfBcYAUwlF8yfTxF3nckLhdSHwOOHXfUPL1n9tG8LZPxdFZ//UnUG0EXAj\noZ7xOeE7UL9lle57lLqvPxBaELOBsYSaxyNpYmksTmkhFrojMyxkNgA4hdC8e5/QXbAj4T9cBaFY\nd5y7f51m3RrCaWorgWXu3j1PsYtkxcx+Adzm7ttnXDjztoYDH7j7NblH1uA+WhMS72Ge40AvkWxk\nbBFEfYZnAbu5+48JZ4ocC9wNXOjuPyH8QmmouboSqHL3XZUEpNDMbG3CGSp35WFbXQmnf96X67bS\nbLtX1CXVjnDG1WLgn/nej0g62XYNtQYqzKwNsA6hyLOdu4+P5o8FjmhgXWvCfkTyJjq75wtCq/X2\nHLd1PeHAfKW7L8hDePX9jHBa8MfAAcBvs6hniORFtl1DZxOKhN8Qzvc+wczGA//t7k+b2XnAFR4G\n89Rfdxahv3IFoV/w7ry+AxERyUnGaw2ZWSWh6NeJ0Nc/ysLQ95OBW8zsMsJZD0sb2MQ+7r7AzDYG\nXjSzaSktidT9qDgkItJE7p7zNbKy6bI5EJgVnY++gnB62d7uPsPdD3b3PQin2KUdPFTXjHb3zwi1\nhAbrBHGPrsvH44orrkjEPvOxzeZsoynrZLtspuVynV8qj7jeRzF+P0vlu5lpmXzJJhHUAj3MrF10\nfnhPYFr0Cx8LFxi7lHAa4Gqic4Y7RM8rCINQpuQr+GJUVVWViH3mY5vN2UZT1sl22UzLZZpfU1OT\n1X6KXRzfzZbab67bLJXvZlP321zZ1giuIAyiWUY4X/xUwvVrziCc7/uEu/85WrYjcLe7HxYN8Hky\nWqYN4doz6c5Vx8w8nxlOJF/69evH/fffH3cYImswMzwPXUNZJYJCUCKQYlVdXR3br2mRxigRiIiU\nuXwlAp3fL5JBdXV13CGItCglAhGRMqeuIRGREqWuIRERyQslApEMVCOQpFMiEBEpc6oRiIiUKNUI\nREQkL5QIRDJQjUCSTolARKTMqUYgIlKiVCMQEZG8UCIQyUA1Akk6JQIRkTKnGoGISIlSjUBEsjZ6\nNFxzDXz7bdyRSDFSIhDJoJRrBHPmwG9+AxdeCG+9Bd26wYQJcUclxUaJQCSBli+Hv/8ddt0VdtsN\n3nsPnngitAqOOALOP1+tA1lFNQKRhJk4EU47DX7wA7jjDth++9Xn//vfcOaZMHky3Hcf7L13PHFK\n7nTPYhFZzaJF8F//BY8/DjfcAMcdB9bIIeLxx0NC6NsX/vY3WGedwsUq+aFisUiBFHuNwB0eewx2\n2gmWLIEPPoDjj288CUDoInr/fZg3T7WDctcm7gBEpPn+9S844wyorYWRI2HffZu2/kYbwYgRoXVw\nxBGhFXHVVWodlBu1CEQyqKqqijuENSxbBtdfD3vsAfvtB2+/3fQkkKqudTB3rloH5Ug1ApESM2FC\nKAZvuSXcdhtss01+t19XO1DroPipWCxSINXV1UXRKvjyS7j4YhgzBgYNgqOOylwHaK58nFn0zTeh\n62rWLJg5M/w7dy786lehhrHWWvmPu9zkKxGoRiBS5FasgEcegT/9CQ4/HKZOhfXXb9l9ZlM7WLkS\nPv549QN96mPhQujSJbRY6h7du4fE8te/hqTWrx+0bduy70UyU4tApEjNmwdDh8I998AWW8BNN8Ge\nexY+jtTWwcEHrzrQ19TAeuuFA/y2265+wN9mG+jYEVo1UIWcMCEklilT4KKL4NRToV27gr6tRCho\n15CZDQBOAVYC7wMnATsCdwAVQA1wnLt/nWbdXsBNhML0UHe/voF9KBFI2VuxAv7xDxgyBF59Ffr0\ngd//PhRw4zZmDHz44aqDfpcuUFGR2zYnTgxjGN56Cy64INQ+ct1mOSlYIjCzzYHxwA7uvtTMRgLP\nAWcA57n7eDPrB2zj7pfXW7cVMAPoCcwHJgF93H16mv0oEUhRKkSNIPXXf8eO4eDfp0/5HBTfeSck\nhFdfhQEDwimx664bd1TFr9ADyloDFWbWBlgHmAds5+7jo/ljgSPSrNcd+NDdZ7v7MmAE0DvHmEUS\nYcUKePZZ6N0bdtkl9Lc//TS8+Sacckr5JAEILZ5Ro+Cll+Ddd0OL46qrQp1BWl7GRODu84GBQC0h\nASxy97HAB2b262ixo4Et06y+BTAnZXpu9JpIych3a2DevFAs7dIl/PvrX4erhN5+e3F0AcXpRz8K\nhfHx4+Gjj6BrV7jsMvj887gjS7aMZw2ZWSXhV3wnYBEwysz6AicDt5jZZcDTwNJcg+nXrx+dO3cG\noLKykm7dun3/n7BumL+mNV2K0y+9VM3EifDGG1W8+irst181l18Op55aHPEV2/SCBdWcdBJcfnkV\n114LnTtX86tfwU03VbHJJtltzx169Khi8WIYO7aab7+FnXeu4uuvYd68arbaqnjeb7bTdc9ramrI\np2xqBEcCB7t7/2j6BGBPdz8zZZntgGHu3qPeuj2Av7h7r2j6YsDTFYxVI5BiVZ1jjeC228Io4HLs\n+8+X2bPDZzhiRLi/Qtu2sHgxfP11+Df1eepra60VPuu6R4cO4d+pU+HII0OLbKON4n53zVfIcQS1\nQA8zawcsIRR+J5nZxu7+WVQQvhS4M826k4CuZtYJWAD0AY7NNWiRUjFyZLgvwNNPq9snF506ha6z\nuqurtmmz6qCeeoCv/7xNA0e4zz+Hv/wFdtwxdD2dfnp5D3DL9vTRKwgH8WXAZOBU4HTCmUMOPOHu\nf46W7Qjc7e6HRdO9gMGsOn30ugb2oRaBJMrUqbD//vDii0oCxWrKFDj3XJg/P4zTOOiguCNqGl1i\nQqSIffVVGEV74YVw0klxRyONcQ8ttvPOg513hoEDYbvt4o4qO7ofgUiBpBbqsuEeTv/cbz8lgVJg\nFk7hnToV9tkH9torJPD//CfuyApHiUAkz266KVyC4eab445EmqJt23C5i/ffD5fV+OEP4d57wzWV\nkk5dQyJ59Oqr4WyUN9+E6ExoKVGTJsE558DSpTB4cGgtFBt1DYkUmY8/DqeGPvCAkkAS7LEHvPZa\nqB306RPu7TxnTub1SpESgUgG2dQIli2DY46B/v2hV6+Wj0kKwywkgOnTwyjnbt3gyivDvRaSRIlA\nJA8uuQTat4fLL8+8rJSeioow+Oztt0NReccdw3WikkI1ApEcjRoVbhrz1luw4YZxRyOFMG5cOCPs\nsMPghhviu52nagQiReD//g/++Ed47DElgXJywAHh0tlffAG77x6elzIlApEMGqoRfP11uHXk1VfD\nT39a2JgkfpWV8PDD4bIXBx0ULiVSqqeaKhFIYq1YAYsWtcy23cMF5Lp3D7dZlPJ13HHhdOHHHw+3\n8pw3L+6Imk6JQBJn7txV1/vfcsvQdVNb2/ztpbvy6K23wrRp4UJolnMPrZS6Ll3g5ZfhZz+D3XaD\nJ5+MO6KmUSKQREi929ePf7zqbl8zZ4YbrO+6azi1c9as3Pf1+uvh7lmPPx5fkVCKT5s24Uqmo0eH\n+y/37x+6D0uBEoGUtNRf/3V3+6qtXXW3r002geuugxkzwv0AuneH3/0uFHmzlVoj+PRTOProcOmB\nbbbJ//uR0tejRygeL1sWWgeTJsUdUWZKBFJyGvr1X3ev3w4d1lxnww1Doqi7/eG++8Kxx4bLEGdr\n+fIwwvR3vwunDYo0ZN114f77Q8vx0EPh2mvD97ZYaRyBlIy5c8Mv8XvuWXW3r2OOSX/gz+Srr0Kr\nYdCgcA2Zyy7LfM+ASy4JYwX+8Q9o3bp570HKT20tnHhiOMFg2DDYeuv8bVvjCKQsNOfXfzbWXTdc\naXLmzNA6OOSQ0K3UUDN+9OhwquDDDysJSNNsvTW89BL88pfhNOORI+OOaE1qEUhRcg8jNm+9Nfdf\n/9n49lsYOjTcF3fnnUMLoe5qkw89VM1551XxzDOw554ts38pD2+9Fa5dtNdecMst4USGXOgOZZJo\nkybBUUfBU08V9jaPS5aEq4dee20oQF90EZx+ejUXXFDFH/9YuDgkuRYvhgEDQiuhuhq22qr521Ii\nkEQ744zQErj00nj2v2wZDB8O11wTfr3df7/GC0h+VVeHcQetcuigVyKQxPruuzAQ7O2381tYa466\nr6SSgBQjFYslsZ55JnQHxZ0EICSAl1+ujjsMkRalRCBF5/77oV+/uKMQKR/qGpKismAB7LRTGDNQ\nURF3NCLFTV1DkkgPPQRHHKEkIFJISgRSNNyLs1som3sWi5QyJQIpGm+9Fc7jrxvIJSKFoRqBFI24\nxw6IlBqNI5BEKaaxAyKlQsViSZRiGjtQn2oEknRZJQIzG2BmU8zsPTMbbmZrm9lPzOx1M5tsZhPN\nLO3tu82sxszerVsuv+FLUhRjkVikXGTsGjKzzYHxwA7uvtTMRgLPAX2Bge7+gpn9ErjQ3Q9Is/4s\nYHd3/zLDftQ1VKY0dkCkefLVNdQmy+VaAxVmthJoD8wDVgLrR/Mro9fSMdQFJY3Q2AGReGU8QLv7\nfGAgUEs42C9097HAAOBGM6sF/hu4pKFNAC+a2SQz65+fsCUpinXsQCrVCCTpMrYIzKwS6A10AhYB\nj5nZcUB34Bx3f8rMjgTuBX6RZhP7uPsCM9uYkBCmufv4dPvq168fnTt3BqCyspJu3bpRVVUFrPrP\nqOlkTVdUVLFkCSxbVk11dfzxaFrTxTxd97ympoZ8yqZGcCRwsLv3j6ZPAHoAfd19g5TlFrn7+g1s\npm6ZK4Cv3P3vaeapRlCGNHZApPkKefpoLdDDzNqZmQE9ganAfDPbPwqmJzAjTZDtzaxD9LwCOAiY\nkmvQkgzffRfu33riiXFHIlLesqkRTARGAZOBdwnF3yHA74GBZjYZ+Fs0jZl1NLMx0eqbAuOjZd4A\nnnH3F/L+LqQkFfPYgVSpzXKRJNLIYonNoYfCscfC8cfHHUnjqqurv++rFSkmusSElDSNHRDJnS4x\nISVNYwdEiocSgRRcKYwdSKUagSSdEoEUnO47IFJcVCOQgtPYAZH8ULFYSpLuOyCSPyoWS0kqlbED\nqVQjkKRTIpCCKqUisUi5UNeQFIzGDojkl7qGpORo7IBIcVIikIIotbEDqVQjkKRTIpCC0NgBkeKl\nGoEUhMYOiOSfxhFIydDYAZGWoWKxlIxSHDuQSjUCSTolAmlxpVokFikX6hqSFqWxAyItR11D0mJG\njoRTToGxY2HFity2pbEDIsVPiUBWs3QpXHABbLwxXHwxbLUVnHce/POfYSxAU5Ty2IFUqhFI0ikR\nyGqGDYMdd4Trrgvn/o8bBx06wNFHh9evugpmzsxuWxo7IFIaVCOQ7y1fHg7299wD+++/+jx3ePNN\nGD4cHn0UttkGjjsuJIhNNkm/PY0dEGlZGkcgeTdiBNxyC4wfD9bIV2vZslA/GD4cxoyBvfcOSaF3\n79B6AI0dECkEJQLJq5Urw7n+110HhxyS/XqLF8Po0SEpvPYaHHpoSAoLF8K994aEUeqqq6upqqqK\nOwyRNeQrEbTJRzBS+p59Flq3hl/+smnrVVRA377h8dlnodvob3+DN94I9QYRKX5qEQjusNdecP75\ncNRR+dnm/Pmw2WbQSqcjiLQYtQgkb8aNC105hx+ev21uvnn+tiUiLUu/14Srrw5jBlq3jjuS4qRx\nBJJ0SgRl7o034KOPQoFXRMpTVonAzAaY2RQze8/MhpvZ2mb2EzN73cwmm9lEM/tpA+v2MrPpZjbD\nzC7Kb/iSq2uugQsvhLXWijuS4qUzhiTpMhaLzWxzYDywg7svNbORwHNAX2Cgu79gZr8ELnT3A+qt\n2wqYAfQE5gOTgD7uPj3NflQsLrD33oODD4ZZs2CddeKORkSaqtAXnWsNVJhZG6A9MA9YCawfza+M\nXquvO/Chu89292XACKB3biFLvlx7LQwYoCSQiWoEknQZzxpy9/lmNhCoBb4BXnD3sWY2F3g+mmfA\n3mlW3wKYkzI9l5AcJGYffhgGew0ZEnckIhK3jInAzCoJv+I7AYuAx8zsOMIB/Rx3f8rMjgTuBX6R\nSzD9+vWjc+fOAFRWVtKtW7fv+2frfpVpOj/T555bzSGHwLrrFkc8xTxdVVVVVPFounyn657X1NSQ\nT9nUCI4EDnb3/tH0CUAPoK+7b5Cy3CJ3X7/euj2Av7h7r2j6YsDd/fo0+1GNoEDmzIGf/CS0Cjbc\nMO5oRKS5ClkjqAV6mFk7MzNC4XcqMN/M9o+C6UkoCtc3CehqZp3MbG2gD/B0rkFLbm68EU4+WUkg\nW6m/xkSSKJsawUQzGwVMBpZF/w4B3gEGm1lr4Dvg9wBm1hG4290Pc/cVZnYm8AIh6Qx192kt81Yk\nG59+Gq4B9MEHcUciIsVC1xoqM3/+c7icxO23xx2JiORKl6GWJlu4ELbdNtw5rEuXuKMRkVzp5vXS\nZLfeCocdpiTQVKoRSNLp6qNlYvFiuPlmePnluCMRkWKjrqEyMWhQuIPYqFFxRyIi+aIagWRtyZJQ\nGxg9GnbfPe5oRCRfVCOQrD3wAOyyi5JAc6lGIEmnGkHCLV8O118P998fdyQiUqzUNZRww4fDXXfB\nK6/EHYmI5JvuWSwZrVwZLjU9cGDckYhIMVONIMGefhratYODDoo7ktKmGoEknRJBQrmHm9L/+c9g\nOTccRSTJVCNIqBdfhHPOgSlToJXSvUgi6fRRadTVV8MllygJiEhmOkwk0GuvwezZ0KdP3JEkg2oE\nknRKBAl0zTVw0UWw1lpxRyIipUA1ggT55BO47TYYOhRmzgxnDIlIcqlGIN/74AM49VTYYYeQDKqr\nlQREJHtKBCXKHZ5/Hg4+GA48EDp3Djejv+su2G67uKNLFtUIJOk0srjEfPdduGzEoEHQujUMGBAG\njrVtG3dkIlKqVCMoEZ9+Gu4zfOed4Sqi550HP/+5BouJlDPVCMpEXf//D38ICxbAuHHw7LPQs6eS\ngIjkhxJBEXKHF16AXr3W7P/fcce4oys/qhFI0qlGUGSGDQv3D6jr/x89Wv3/ItKyVCMoIkOGhEtG\n3367+v9FJDPdszhhJkyA3/wGxo+H7bePOxoRKQUqFifI/Plw9NFw331KAsVINQJJOiWCmC1ZAocf\nDqefDoceGnc0IlKO1DUUI/dwauiiRfDYY6oJiEjTFPSexWY2ADgFWAm8D5wMPADUdWRsAHzp7rul\nWbcGWBStu8zdu+cadFLccQdMnAivv64kICLxyZgIzGxz4CxgB3dfamYjgWPcvU/KMjcCCxvYxEqg\nyt2/zEfASfHqq3DllaFI3KFD3NFIY6qrq6mqqoo7DJEWk+04gtZAhZmtBNoD8+vNPxo4oIF1DdUi\nVjNnDhxzDDz4IGy7bdzRiEi5y3iAdvf5wECgFpgHLHT3sXXzzWw/4GN3n9nQJoAXzWySmfXPQ8wl\n7dtvQ3H43HPDlUOl+Kk1IEmXTddQJdAb6ETo6x9lZn3d/eFokWOBRxrZxD7uvsDMNiYkhGnuPj7d\ngv369aNz584AVFZW0q1bt+//E9adwlfK0+7wwANVbLst7LFHNdXVxRWfpjWt6eKernteU1NDPmU8\na8jMjgQOdvf+0fQJwJ7ufqaZtSa0EnaLWg6ZtnUF8JW7/z3NvMSfNXTzzeHuYRMmQEVF3NFItqpV\nI5AiVcgBZbVADzNrZ2YG9ASmRfN+AUxrKAmYWXsz6xA9rwAOAqbkGnQpGjcu3Ev4qaeUBESkuGRT\nI5gIjAImA+8Sir9DotnHUK9byMw6mtmYaHJTYLyZTQbeAJ5x9xfyFHvJmD0b+vYNN5Tp0iXuaKSp\n1BqQpNOAshb2zTew775w/PHhZjIiIvmiaw2VAHfo3x922ilcUlpKU2qhTiSJdD+CFjRoEEyfHq4o\nqpHDIlKs1DXUQsaOhRNOgDffhK23jjsaEUmigl5rSJpm1iw47jgYOVJJQESKn2oEebZ4cbjBzKWX\ngk42SQbVCCTplAjyyB1OPhl23x3OPDPuaEREsqMaQZ6sXAlXXAHPPw+vvALt2sUdkYgknWoEReSj\nj0JLYMUKePJJJQERKS3qGsrBypXh+kE9esBvfxtaAltsEXdUkm+qEUjSqUXQTKmtgAkTdNN5ESld\nahE0UbpWgJJAsulaQ5J0ahE0gVoBIpJEahFkQa2A8qYagSSdWgQZqBUgIkmnFkED1AqQOqoRSNKp\nRZCGWgEiUk7UIkihVoCkoxqBJJ1aBJE5c8IVQ9UKEJFyo2sNRfr2hc02gxtugNatYwtDRCRr+brW\nkBIBMG8e7LJLuI9AZWUsIYiINJnuWZxHd94ZWgRKApKOagSSdGVfI/juOxgyJBSGRUTKUdl3Dd13\nHzz6KPzP/xR81yIiOVHXUB64w+DBcM45cUciIhKfsk4Er7wSuoYOOijuSKSYqUYgSVfWieDmm+Hs\ns6FVWX8v7mI8AAAHtUlEQVQKIlLuyrZGUFMTbjI/ezZ06FCw3YqI5I1qBDm67Tbo109JQEQkq0Rg\nZgPMbIqZvWdmw82srZmNMLO3o8e/zOztBtbtZWbTzWyGmV2U3/CbZ/HicLbQmWfGHYmUAtUIJOky\njiMws82Bs4Ad3H2pmY0EjnH3PinL3AgsTLNuK+BWoCcwH5hkZqPdfXq+3kBzPPgg7LcfdOkSZxQi\nIsUh266h1kCFmbUB2hMO6qmOBh5Js1534EN3n+3uy4ARQO/mBpsP7quKxCLZ0P0IJOkyJgJ3nw8M\nBGqBecBCdx9bN9/M9gM+dveZaVbfApiTMj03ei02L74Ia68N+r8tIhJk0zVUSfgV3wlYBIwys77u\n/nC0yLGkbw00Wb9+/ejcuTMAlZWVdOvW7ftfY3X9tLlODx5cxdlnw8sv52d7mk7+dGqNoBji0XT5\nTtc9r6mpIZ8ynj5qZkcCB7t7/2j6BGBPdz/TzFoTWgm7RS2H+uv2AP7i7r2i6YsBd/fr0yzb4qeP\nzpgB++4bThldZ50W3ZUkSHV19ff/IUWKSSFPH60FephZOzMzQuF3WjTvF8C0dEkgMgnoamadzGxt\noA/wdK5BN9ett0L//koC0jRKApJ0GbuG3H2imY0CJgPLon+HRLOPoV63kJl1BO5298PcfYWZnQm8\nQEg6Q919GjFYtAgeegjeey+OvYuIFK+yGVl8003wxhswYkSL7UISSl1DUqzy1TVUFvcjWLECbrkl\ntAhERGR1ZXGJiWefhQ03hB494o5ESpFaA5J0ZZEI6gaQWc4NKBGR5El8IpgyBaZOhaOPjjsSKVWp\n53CLJFHiE8HNN8Mf/hBGE4uIyJoSfdbQ559D164wfTpsumleNy0iEjvdjyALd98NvXsrCYiINCax\niWD58nDzGV1lVHKlGoEkXWITwZNPQufOsNtucUciIlLcElsj2HdfOPdcOPLIvG1SRKSoqEbQiH/+\nE2pr4Te/iTsSEZHil8hEMHgwnHEGtCmLC2hIS1ONQJIucYng44/hmWfC5aZFRCSzxNUIrrwS5s+H\nu+7KQ1AiIkUsXzWCRCWCJUvCmUJjx8LOO+cnLhGRYqVicRqPPgo/+pGSgOSXagSSdIlJBO6hSKwB\nZCIiTZOYrqEJE+DEE8MN6lslJr2JiDRMXUP1DB4MZ52lJCAi0lQl0yJYuRK+/BI++QQ+/TT8m/r8\n8cehpgbWW69wMUt50D2LpVgl8p7Fw4ateaCvm/7sM6ioCFcS3XRT2GSTVc/32ANOP11JQESkOYqq\nRdC3r6c90G+6KWy8MbRtG3eUIiLFQ+MIRETKnIrFIgWicQSSdEoEIiJlTl1DIiIlSl1DIiKSF0oE\nIhmoRiBJl1UiMLMBZjbFzN4zs+Fmtnb0+llmNs3M3jez6xpYt8bM3jWzyWY2MZ/BixTCO++8E3cI\nIi0q44AyM9scOAvYwd2XmtlIoI+Z1QK/AnZx9+VmtlEDm1gJVLn7l3mLWqSAFi5cGHcIIi0q266h\n1kCFmbUB2gPzgdOB69x9OYC7/7uBda0J+yl5cXQjtMQ+87HN5myjKetku2ym5cql6yeu91mM389S\n+W42db/NlfEA7e7zgYFALTAPWOjuY4HtgZ+Z2RtmNs7MftrQJoAXzWySmSX+BpJKBLltoxgTQU1N\nTVb7KXZKBLmtn+REgLs3+gAqgZeAHxBaBk8AxwHvA4OjZfYAZjWwfsfo342Bd4B9G1jO9dBDDz30\naNoj0zE8m0c2F507kHCQ/wLAzJ4E9gbmEJIC7j7JzFaa2Ybu/nnqyu6+IPr3s2jd7sD4+jvJx7mw\nIiLSdNn03dcCPcysnZkZ0BOYCjwF/BzAzLYH1qqfBMysvZl1iJ5XAAcBU/IYv4iI5Chji8DdJ5rZ\nKGAysCz6d0g0+14zex9YApwIYGYdgbvd/TBgU+BJM/NoX8Pd/YX8vw0REWmuornEhIiIxKNsTusU\nEZH0lAhERMpc0SeCqOA8ycwOiTsWkTpmtoOZ3WFmj5rZH+KORySVmfU2syFm9oiZ/SLj8sVeIzCz\nK4GvgKnu/lzc8Yikis6ke8DdT4w7FpH6zKwSuMHdGx3MW5AWgZkNNbNPzOy9eq/3MrPpZjbDzC5K\ns96BhFNVPyNcqkIkr5r73YyW+RUwBtAPFGkRuXw/I5cCt2XcTyFaBGa2L/A18KC7/zh6rRUwgzAu\nYT4wCejj7tPN7ARgN2A9YBGwM/CNu/+2xYOVstLM7+auhF9ZC6Llx0SnS4vkVQ7fzxuBs4EX3P1/\nM+0nm5HFOXP38WbWqd7L3YEP3X02gJmNAHoD0919GDCsbkEzOxFo6KJ2Is3W3O+mme1vZhcDbYFn\nCxq0lI0cvp9nERLFembW1d2H0IiCJIIGbEG4TEWduYQ3uAZ3f7AgEYkEGb+b7v4y8HIhgxKJZPP9\nvAW4JdsNFv1ZQyIi0rLiTATzgK1TpreMXhOJm76bUszy/v0sZCIwVj/zZxLQ1cw6Rbe+7AM8XcB4\nROrouynFrMW/n4U6ffRhYAKwvZnVmtlJ7r6CcAvMF4APgBHuPq0Q8YjU0XdTilmhvp9FP6BMRERa\nlorFIiJlTolARKTMKRGIiJQ5JQIRkTKnRCAiUuaUCEREypwSgYhImVMiEBEpc/8PEVcWrDZ0qD0A\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12d516630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's plot the accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogx(params_regularization, accuracy_values)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the technique with 2-layer network (1-hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/cpu:0'):   # for force CPU only (as my GPU mem was not enough)\n",
    "        # Input data\n",
    "        tf_train_dataset = tf.placeholder( tf.float32, \n",
    "                                         shape=(batch_size, image_size*image_size))\n",
    "        tf_train_labels  = tf.placeholder( tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset  = tf.constant(test_dataset)\n",
    "        # Beta for Regularization\n",
    "        beta             = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Variables\n",
    "        weights1 = tf.Variable( \n",
    "            tf.truncated_normal([image_size*image_size, num_hidden_nodes]))\n",
    "        biases1  = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "        weights2 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "        biases2  = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "        # Training computation\n",
    "        layer1_out = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "        logits = tf.matmul(layer1_out, weights2) + biases2\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "        \n",
    "        # Preductions for training / validation / test\n",
    "        train_prediction  = tf.nn.softmax(logits)\n",
    "        layer1_validation = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "        valid_prediction  = tf.nn.softmax(tf.matmul(layer1_validation, weights2) + biases2)\n",
    "        layer1_test       = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "        test_prediction   = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "minibatch loss at step 0: 663.689575\n",
      "minibatch accuracy : 10.0%\n",
      "validation accuracy: 29.5%\n",
      "minibatch loss at step 500: 204.714737\n",
      "minibatch accuracy : 77.0%\n",
      "validation accuracy: 79.4%\n",
      "minibatch loss at step 1000: 115.530586\n",
      "minibatch accuracy : 75.0%\n",
      "validation accuracy: 80.6%\n",
      "minibatch loss at step 1500: 69.627472\n",
      "minibatch accuracy : 76.0%\n",
      "validation accuracy: 81.2%\n",
      "minibatch loss at step 2000: 41.660244\n",
      "minibatch accuracy : 77.0%\n",
      "validation accuracy: 83.0%\n",
      "minibatch loss at step 2500: 25.039907\n",
      "minibatch accuracy : 90.0%\n",
      "validation accuracy: 85.8%\n",
      "minibatch loss at step 3000: 15.567118\n",
      "minibatch accuracy : 85.0%\n",
      "validation accuracy: 86.2%\n",
      "\n",
      "Test accuracy: 93.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "\n",
    "beta_fixed = 1e-3   # Testing with one value of Regularization param\n",
    "\n",
    "## For limiting GPU memory usage: -- Added by Atul\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a mini batch\n",
    "        batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "        # Make a dict telling the session where to find the minibatch\n",
    "        feed_dict = { tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: beta_fixed }    # Regularization param from above\n",
    "        # Run the damn thing\n",
    "        _, loss_step, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        # show progress\n",
    "        if (step % 500 == 0):\n",
    "            print(\"minibatch loss at step %d: %f\" % (step, loss_step))\n",
    "            print(\"minibatch accuracy : %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Getting better. Let's try for different value of Regularization param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized for reg param:  0.0001\n",
      "Initialized for reg param:  0.000125892541179\n",
      "Initialized for reg param:  0.000158489319246\n",
      "Initialized for reg param:  0.000199526231497\n",
      "Initialized for reg param:  0.000251188643151\n",
      "Initialized for reg param:  0.000316227766017\n",
      "Initialized for reg param:  0.000398107170553\n",
      "Initialized for reg param:  0.000501187233627\n",
      "Initialized for reg param:  0.00063095734448\n",
      "Initialized for reg param:  0.000794328234724\n",
      "Initialized for reg param:  0.001\n",
      "Initialized for reg param:  0.00125892541179\n",
      "Initialized for reg param:  0.00158489319246\n",
      "Initialized for reg param:  0.00199526231497\n",
      "Initialized for reg param:  0.00251188643151\n",
      "Initialized for reg param:  0.00316227766017\n",
      "Initialized for reg param:  0.00398107170553\n",
      "Initialized for reg param:  0.00501187233627\n",
      "Initialized for reg param:  0.0063095734448\n",
      "Initialized for reg param:  0.00794328234724\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "params_regularization = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_values = []\n",
    "\n",
    "for R_param in params_regularization:\n",
    "    with tf.Session(graph=graph, config=config) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print (\"Initialized for reg param: \", R_param)\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a mini batch\n",
    "            batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "            # Make a dict telling the session where to find the minibatch\n",
    "            feed_dict = { tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: R_param }\n",
    "            # Run the damn thing\n",
    "            _, loss_step, predictions = session.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            # save the TEST accuracy only\n",
    "        accuracy_values.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEOCAYAAACD5gx6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXkSkTCnBi4iUqf8Hrl+RLoe9eFEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137f695f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot it\n",
    "plt.semilogx(params_regularization, accuracy_values)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-hidden RELU layer)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of max accuracy:  11\n",
      "max accuracy:  93.33 , reg param: 0.00125892541179\n"
     ]
    }
   ],
   "source": [
    "def myargmax(lst):\n",
    "    return lst.index(max(lst))\n",
    "\n",
    "maxindex = myargmax(accuracy_values)\n",
    "print('index of max accuracy: ', maxindex)\n",
    "print('max accuracy: ', accuracy_values[maxindex], ', reg param:', params_regularization[maxindex] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's define a 2-layer network with 1-hidden ReLU layer\n",
    "batch_size = 100\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/cpu:0'):   # for force CPU only (as my GPU mem was not enough)\n",
    "        # Input data\n",
    "        tf_train_dataset = tf.placeholder( tf.float32, \n",
    "                                         shape=(batch_size, image_size*image_size))\n",
    "        tf_train_labels  = tf.placeholder( tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset  = tf.constant(test_dataset)\n",
    "        # Beta for Regularization\n",
    "        beta             = tf.placeholder(tf.float32)\n",
    "        \n",
    "        # Variables\n",
    "        weights1 = tf.Variable( \n",
    "            tf.truncated_normal([image_size*image_size, num_hidden_nodes]))\n",
    "        biases1  = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "        weights2 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "        biases2  = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "        # Training computation\n",
    "        layer1_out = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "        logits = tf.matmul(layer1_out, weights2) + biases2\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "        \n",
    "        # Preductions for training / validation / test\n",
    "        train_prediction  = tf.nn.softmax(logits)\n",
    "        layer1_validation = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "        valid_prediction  = tf.nn.softmax(tf.matmul(layer1_validation, weights2) + biases2)\n",
    "        layer1_test       = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "        test_prediction   = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "minibatch loss at step 0: 733.867920\n",
      "minibatch accuracy : 12.0%\n",
      "validation accuracy: 29.5%\n",
      "minibatch loss at step 5: 445.479431\n",
      "minibatch accuracy : 82.0%\n",
      "validation accuracy: 59.1%\n",
      "minibatch loss at step 10: 311.561920\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 15: 310.007568\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 20: 308.461121\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 25: 306.922333\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 30: 305.391235\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 35: 303.867859\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 40: 302.351837\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 45: 300.843262\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 50: 299.342682\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 55: 297.849121\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 60: 296.363251\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 65: 294.884827\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 70: 293.413391\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 75: 291.949951\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 80: 290.493347\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 85: 289.044220\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 90: 287.602264\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 95: 286.167542\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "minibatch loss at step 100: 284.739777\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 58.8%\n",
      "\n",
      "Test accuracy: 66.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101   # few steps to show Overfitting\n",
    "num_batches = 5   # Using small batches to show Overfitting\n",
    "\n",
    "\n",
    "beta_fixed = 1e-3   # Testing with one value of Regularization param\n",
    "\n",
    "## For limiting GPU memory usage: -- Added by Atul\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = step % num_batches\n",
    "        # Generate a mini batch\n",
    "        batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "        # Make a dict telling the session where to find the minibatch\n",
    "        feed_dict = { tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: beta_fixed }    # Regularization param from above\n",
    "        # Run the damn thing\n",
    "        _, loss_step, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        # show progress\n",
    "        if (step % 5 == 0):\n",
    "            print(\"minibatch loss at step %d: %f\" % (step, loss_step))\n",
    "            print(\"minibatch accuracy : %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it shows, the generalization accuracy is terrible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Let's intro a Dropout in Training\n",
    "## Let's define a 2-layer network with 1-hidden ReLU layer\n",
    "batch_size = 100\n",
    "num_hidden_nodes = 1024\n",
    "#dropout_percent = 0.5   # 50% dropout\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/cpu:0'):   # for force CPU only (as my GPU mem was not enough)\n",
    "        # Input data\n",
    "        tf_train_dataset = tf.placeholder( tf.float32, \n",
    "                                         shape=(batch_size, image_size*image_size))\n",
    "        tf_train_labels  = tf.placeholder( tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset  = tf.constant(test_dataset)\n",
    "        # Beta for Regularization\n",
    "        beta             = tf.placeholder(tf.float32)\n",
    "        keep_prob        = tf.placeholder(tf.float32)   # for Dropout\n",
    "        \n",
    "        # Variables\n",
    "        weights1 = tf.Variable( \n",
    "            tf.truncated_normal([image_size*image_size, num_hidden_nodes]))\n",
    "        biases1  = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "        weights2 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "        biases2  = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "        # Training computation.... with Dropout\n",
    "        layer1_out = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "        # Dropout some output of Layer 1... only on Training data!\n",
    "        drop1      = tf.nn.dropout(layer1_out, keep_prob)\n",
    "        logits     = tf.matmul(drop1, weights2) + biases2\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "        \n",
    "        # Preductions for training / validation / test; no Dropout on Validation/Test\n",
    "        train_prediction  = tf.nn.softmax(logits)\n",
    "        layer1_validation = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "        valid_prediction  = tf.nn.softmax(tf.matmul(layer1_validation, weights2) + biases2)\n",
    "        layer1_test       = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "        test_prediction   = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "minibatch loss at step 0: 793.473877\n",
      "minibatch accuracy : 10.0%\n",
      "validation accuracy: 24.9%\n",
      "minibatch loss at step 5: 349.102173\n",
      "minibatch accuracy : 90.0%\n",
      "validation accuracy: 59.6%\n",
      "minibatch loss at step 10: 318.344818\n",
      "minibatch accuracy : 96.0%\n",
      "validation accuracy: 61.8%\n",
      "minibatch loss at step 15: 312.235687\n",
      "minibatch accuracy : 99.0%\n",
      "validation accuracy: 65.7%\n",
      "minibatch loss at step 20: 309.521851\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 66.3%\n",
      "minibatch loss at step 25: 309.919678\n",
      "minibatch accuracy : 98.0%\n",
      "validation accuracy: 65.4%\n",
      "minibatch loss at step 30: 307.983002\n",
      "minibatch accuracy : 99.0%\n",
      "validation accuracy: 64.5%\n",
      "minibatch loss at step 35: 305.733612\n",
      "minibatch accuracy : 99.0%\n",
      "validation accuracy: 64.6%\n",
      "minibatch loss at step 40: 303.655579\n",
      "minibatch accuracy : 99.0%\n",
      "validation accuracy: 64.1%\n",
      "minibatch loss at step 45: 301.962982\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 64.0%\n",
      "minibatch loss at step 50: 301.093658\n",
      "minibatch accuracy : 99.0%\n",
      "validation accuracy: 63.4%\n",
      "minibatch loss at step 55: 298.978577\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 64.5%\n",
      "minibatch loss at step 60: 300.978455\n",
      "minibatch accuracy : 99.0%\n",
      "validation accuracy: 66.2%\n",
      "minibatch loss at step 65: 296.009399\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 66.2%\n",
      "minibatch loss at step 70: 294.532867\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 66.2%\n",
      "minibatch loss at step 75: 293.063385\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 66.2%\n",
      "minibatch loss at step 80: 291.601501\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 66.2%\n",
      "minibatch loss at step 85: 290.155701\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 65.7%\n",
      "minibatch loss at step 90: 288.715424\n",
      "minibatch accuracy : 100.0%\n",
      "validation accuracy: 65.8%\n",
      "minibatch loss at step 95: 288.766205\n",
      "minibatch accuracy : 99.0%\n",
      "validation accuracy: 66.4%\n",
      "minibatch loss at step 100: 287.545410\n",
      "minibatch accuracy : 99.0%\n",
      "validation accuracy: 66.4%\n",
      "\n",
      "Test accuracy: 73.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101   # few steps to show Overfitting\n",
    "num_batches = 5   # Using small batches to show Overfitting\n",
    "\n",
    "beta_fixed = 1e-3   # Testing with one value of Regularization param\n",
    "keep_prob_val  = 0.5    # 50% dropout\n",
    "\n",
    "## For limiting GPU memory usage: \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        # offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        offset = step % num_batches\n",
    "        # Generate a mini batch\n",
    "        batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "        # Make a dict telling the session where to find the minibatch\n",
    "        feed_dict = { tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: beta_fixed,    # Reg param from above\n",
    "                     keep_prob: keep_prob_val}    # dropout\n",
    "        # Run the damn thing\n",
    "        _, loss_step, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        # show progress\n",
    "        if (step % 5 == 0):\n",
    "            print(\"minibatch loss at step %d: %f\" % (step, loss_step))\n",
    "            print(\"minibatch accuracy : %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets better accuracy with some 50% Dropout... but still low bcos we did very few batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a deeper network with:\n",
    "- 3 layers (2 hidden)\n",
    "- Regularization\n",
    "- Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's define a 3-layer network with 2-hidden ReLU layer\n",
    "batch_size = 100\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "beta_fixed = 0.00125892541179   # Best Reg param we found. Or else just 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/cpu:0'):   # for force CPU only (as my GPU mem was not enough)\n",
    "        # Input data\n",
    "        tf_train_dataset = tf.placeholder( tf.float32, \n",
    "                                         shape=(batch_size, image_size*image_size))\n",
    "        tf_train_labels  = tf.placeholder( tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset  = tf.constant(test_dataset)\n",
    "        # Beta for Regularization\n",
    "        beta             = tf.placeholder(tf.float32)\n",
    "        # Global Step\n",
    "        global_step      = tf.Variable(0)\n",
    "        \n",
    "        # Variables\n",
    "        weights1 = tf.Variable( \n",
    "            tf.truncated_normal([image_size*image_size, num_hidden_nodes1],\n",
    "                               stddev=np.sqrt(2.0 / (image_size*image_size)) )\n",
    "        )\n",
    "        biases1  = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "        weights2 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2],\n",
    "                               stddev=np.sqrt(2.0/num_hidden_nodes1))\n",
    "        )\n",
    "        biases2  = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "        weights3 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes2, num_labels],\n",
    "                               stddev=np.sqrt(2.0/num_hidden_nodes2))\n",
    "            )\n",
    "        biases3  = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "        # Training computation - Layer1\n",
    "        layer1_out = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "        # Layer 2\n",
    "        layer2_out = tf.nn.relu(tf.matmul(layer1_out, weights2) + biases2)\n",
    "        # Logits \n",
    "        logits = tf.matmul(layer2_out, weights3) + biases3\n",
    "        # Loss\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "        \n",
    "        # Preductions for training / validation / test\n",
    "        train_prediction  = tf.nn.softmax(logits)\n",
    "        layer1_validation = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "        layer2_validation = tf.nn.relu(tf.matmul(layer1_validation, weights2) + biases2)\n",
    "        valid_prediction  = tf.nn.softmax(tf.matmul(layer2_validation, weights3) + biases3)\n",
    "        \n",
    "        layer1_test       = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "        layer2_test       = tf.nn.relu(tf.matmul(layer1_test, weights2) + biases2)\n",
    "        test_prediction   = tf.nn.softmax(tf.matmul(layer2_test, weights3) + biases3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized, with reg: 0.001\n",
      "minibatch loss at step 0: 3.426083\n",
      "minibatch accuracy : 7.0%\n",
      "validation accuracy: 34.7%\n",
      "minibatch loss at step 500: 1.262513\n",
      "minibatch accuracy : 77.0%\n",
      "validation accuracy: 85.0%\n",
      "minibatch loss at step 1000: 1.025840\n",
      "minibatch accuracy : 80.0%\n",
      "validation accuracy: 86.4%\n",
      "minibatch loss at step 1500: 0.906518\n",
      "minibatch accuracy : 81.0%\n",
      "validation accuracy: 86.6%\n",
      "minibatch loss at step 2000: 0.691344\n",
      "minibatch accuracy : 86.0%\n",
      "validation accuracy: 87.6%\n",
      "minibatch loss at step 2500: 0.400855\n",
      "minibatch accuracy : 94.0%\n",
      "validation accuracy: 87.4%\n",
      "minibatch loss at step 3000: 0.623376\n",
      "minibatch accuracy : 89.0%\n",
      "validation accuracy: 87.7%\n",
      "minibatch loss at step 3500: 0.525751\n",
      "minibatch accuracy : 90.0%\n",
      "validation accuracy: 88.3%\n",
      "minibatch loss at step 4000: 0.481462\n",
      "minibatch accuracy : 90.0%\n",
      "validation accuracy: 87.5%\n",
      "minibatch loss at step 4500: 0.514146\n",
      "minibatch accuracy : 86.0%\n",
      "validation accuracy: 87.7%\n",
      "minibatch loss at step 5000: 0.487243\n",
      "minibatch accuracy : 91.0%\n",
      "validation accuracy: 88.7%\n",
      "\n",
      "Test accuracy: 94.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "\n",
    "beta_fixed = 1e-3   # Testing with one value of Regularization param\n",
    "\n",
    "## For limiting GPU memory usage: \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized, with reg:\", beta_fixed)\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a mini batch\n",
    "        batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "        # Make a dict telling the session where to find the minibatch\n",
    "        feed_dict = { tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: beta_fixed }    # Regularization param from above\n",
    "        # Run the damn thing\n",
    "        _, loss_step, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        # show progress\n",
    "        if (step % 500 == 0):\n",
    "            print(\"minibatch loss at step %d: %f\" % (step, loss_step))\n",
    "            print(\"minibatch accuracy : %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already gotten good results. Let's try another layer, with Dropout now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's try a larger network\n",
    "\n",
    "## Let's define a 4-layer network with 3-hidden ReLU layer\n",
    "batch_size = 100\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "beta_fixed = 0.00125892541179   # Best Reg param we found. Or else just 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/cpu:0'):   # for force CPU only (as my GPU mem was not enough)\n",
    "        # Input data\n",
    "        tf_train_dataset = tf.placeholder( tf.float32, \n",
    "                                         shape=(batch_size, image_size*image_size))\n",
    "        tf_train_labels  = tf.placeholder( tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset  = tf.constant(test_dataset)\n",
    "        # Beta for Regularization\n",
    "        beta             = tf.placeholder(tf.float32)\n",
    "        # Global Step\n",
    "        global_step      = tf.Variable(0)\n",
    "        \n",
    "        ## Variables\n",
    "        # layer1\n",
    "        weights1 = tf.Variable( \n",
    "            tf.truncated_normal([image_size*image_size, num_hidden_nodes1],\n",
    "                               stddev=np.sqrt(2.0 / (image_size*image_size)) )\n",
    "        )\n",
    "        biases1  = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "        # layer2\n",
    "        weights2 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2],\n",
    "                               stddev=np.sqrt(2.0/num_hidden_nodes1))\n",
    "        )\n",
    "        biases2  = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "        # layer3\n",
    "        weights3 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3],\n",
    "                               stddev=np.sqrt(2.0/num_hidden_nodes2))\n",
    "            )\n",
    "        biases3  = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "        # layer4\n",
    "        weights4 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes3, num_labels],\n",
    "                               stddev=np.sqrt(2.0/num_hidden_nodes3))\n",
    "            )\n",
    "        biases4  = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "        # Training computation - Layer1\n",
    "        layer1_out = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "        # Layer 2\n",
    "        layer2_out = tf.nn.relu(tf.matmul(layer1_out, weights2) + biases2)\n",
    "        # Layer 3\n",
    "        layer3_out = tf.nn.relu(tf.matmul(layer2_out, weights3) + biases3)\n",
    "        # Logits \n",
    "        logits = tf.matmul(layer3_out, weights4) + biases4\n",
    "        # Loss\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4))\n",
    "        \n",
    "        # Optimizer, with Learning Rate schedule\n",
    "        learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "        # Preductions for training / validation / test\n",
    "        train_prediction  = tf.nn.softmax(logits)\n",
    "        layer1_validation = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "        layer2_validation = tf.nn.relu(tf.matmul(layer1_validation, weights2) + biases2)\n",
    "        layer3_validation = tf.nn.relu(tf.matmul(layer2_validation, weights3) + biases3)\n",
    "        valid_prediction  = tf.nn.softmax(tf.matmul(layer3_validation, weights4) + biases4)\n",
    "        \n",
    "        layer1_test       = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "        layer2_test       = tf.nn.relu(tf.matmul(layer1_test, weights2) + biases2)\n",
    "        layer3_test       = tf.nn.relu(tf.matmul(layer2_test, weights3) + biases3)\n",
    "        test_prediction   = tf.nn.softmax(tf.matmul(layer3_test, weights4) + biases4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized, with reg: 0.001\n",
      "minibatch loss at step 0: 3.721385\n",
      "minibatch accuracy : 13\n",
      "validation accuracy: 19.7\n",
      "minibatch loss at step 500: 1.494225\n",
      "minibatch accuracy : 80\n",
      "validation accuracy: 84.63\n",
      "minibatch loss at step 1000: 1.156099\n",
      "minibatch accuracy : 83\n",
      "validation accuracy: 86.77\n",
      "minibatch loss at step 1500: 1.025510\n",
      "minibatch accuracy : 80\n",
      "validation accuracy: 86.41\n",
      "minibatch loss at step 2000: 0.738874\n",
      "minibatch accuracy : 89\n",
      "validation accuracy: 86.99\n",
      "minibatch loss at step 2500: 0.457955\n",
      "minibatch accuracy : 93\n",
      "validation accuracy: 87.34\n",
      "minibatch loss at step 3000: 0.639746\n",
      "minibatch accuracy : 89\n",
      "validation accuracy: 87.59\n",
      "minibatch loss at step 3500: 0.557891\n",
      "minibatch accuracy : 87\n",
      "validation accuracy: 88.26\n",
      "minibatch loss at step 4000: 0.527442\n",
      "minibatch accuracy : 88\n",
      "validation accuracy: 87.48\n",
      "minibatch loss at step 4500: 0.543994\n",
      "minibatch accuracy : 87\n",
      "validation accuracy: 87.68\n",
      "minibatch loss at step 5000: 0.484738\n",
      "minibatch accuracy : 93\n",
      "validation accuracy: 88.33\n",
      "minibatch loss at step 5500: 0.458188\n",
      "minibatch accuracy : 92\n",
      "validation accuracy: 85.3\n",
      "minibatch loss at step 6000: 0.425169\n",
      "minibatch accuracy : 92\n",
      "validation accuracy: 87.76\n",
      "minibatch loss at step 6500: 0.381000\n",
      "minibatch accuracy : 93\n",
      "validation accuracy: 88.09\n",
      "minibatch loss at step 7000: 0.337569\n",
      "minibatch accuracy : 95\n",
      "validation accuracy: 88.62\n",
      "minibatch loss at step 7500: 0.425215\n",
      "minibatch accuracy : 90\n",
      "validation accuracy: 87.06\n",
      "minibatch loss at step 8000: 0.468743\n",
      "minibatch accuracy : 89\n",
      "validation accuracy: 87.69\n",
      "minibatch loss at step 8500: 0.630635\n",
      "minibatch accuracy : 84\n",
      "validation accuracy: 87.9\n",
      "minibatch loss at step 9000: 0.422280\n",
      "minibatch accuracy : 91\n",
      "validation accuracy: 88.5\n",
      "minibatch loss at step 9500: 0.531500\n",
      "minibatch accuracy : 86\n",
      "validation accuracy: 88.08\n",
      "minibatch loss at step 10000: 0.536900\n",
      "minibatch accuracy : 89\n",
      "validation accuracy: 88.12\n",
      "minibatch loss at step 10500: 0.652748\n",
      "minibatch accuracy : 84\n",
      "validation accuracy: 87.75\n",
      "minibatch loss at step 11000: 0.487509\n",
      "minibatch accuracy : 87\n",
      "validation accuracy: 88.42\n",
      "minibatch loss at step 11500: 0.578097\n",
      "minibatch accuracy : 84\n",
      "validation accuracy: 86.42\n",
      "minibatch loss at step 12000: 0.421849\n",
      "minibatch accuracy : 89\n",
      "validation accuracy: 88.32\n",
      "minibatch loss at step 12500: 0.478495\n",
      "minibatch accuracy : 89\n",
      "validation accuracy: 86.91\n",
      "minibatch loss at step 13000: 0.480607\n",
      "minibatch accuracy : 91\n",
      "validation accuracy: 88.21\n",
      "minibatch loss at step 13500: 0.620631\n",
      "minibatch accuracy : 84\n",
      "validation accuracy: 85.38\n",
      "minibatch loss at step 14000: 0.514108\n",
      "minibatch accuracy : 88\n",
      "validation accuracy: 87.74\n",
      "minibatch loss at step 14500: 0.468657\n",
      "minibatch accuracy : 93\n",
      "validation accuracy: 87.24\n",
      "minibatch loss at step 15000: 0.350874\n",
      "minibatch accuracy : 93\n",
      "validation accuracy: 87.75\n",
      "\n",
      "Test accuracy: 94.04\n"
     ]
    }
   ],
   "source": [
    "## Train and run the network\n",
    "num_steps = 15001\n",
    "\n",
    "beta_fixed = 1e-3   # Testing with one value of Regularization param\n",
    "\n",
    "## For limiting GPU memory usage: \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized, with reg:\", beta_fixed)\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a mini batch\n",
    "        batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "        # Make a dict telling the session where to find the minibatch\n",
    "        feed_dict = { tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: beta_fixed }    # Regularization param from above\n",
    "        # Run the damn thing\n",
    "        _, loss_step, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        # show progress\n",
    "        if (step % 500 == 0):\n",
    "            print(\"minibatch loss at step %d: %f\" % (step, loss_step))\n",
    "            print(\"minibatch accuracy : %g\" % accuracy(predictions, batch_labels))\n",
    "            print(\"validation accuracy: %g\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"\\nTest accuracy: %g\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try yet another - this time with Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's try a larger network\n",
    "\n",
    "## Let's define a 4-layer network with 3-hidden ReLU layer\n",
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 128\n",
    "beta_fixed = 0.00125892541179   # Best Reg param we found. Or else just 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/cpu:0'):   # for force CPU only (as my GPU mem was not enough)\n",
    "        # Input data\n",
    "        tf_train_dataset = tf.placeholder( tf.float32, \n",
    "                                         shape=(batch_size, image_size*image_size))\n",
    "        tf_train_labels  = tf.placeholder( tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset  = tf.constant(test_dataset)\n",
    "        # Beta for Regularization\n",
    "        beta             = tf.placeholder(tf.float32)\n",
    "        # Global Step\n",
    "        global_step      = tf.Variable(0)\n",
    "        keep_prob        = tf.placeholder(tf.float32)   # for Dropout\n",
    "        \n",
    "        ## Variables\n",
    "        # layer1\n",
    "        weights1 = tf.Variable( \n",
    "            tf.truncated_normal([image_size*image_size, num_hidden_nodes1],\n",
    "                               stddev=np.sqrt(2.0 / (image_size*image_size)) )\n",
    "        )\n",
    "        biases1  = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "        # layer2\n",
    "        weights2 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2],\n",
    "                               stddev=np.sqrt(2.0/num_hidden_nodes1))\n",
    "        )\n",
    "        biases2  = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "        # layer3\n",
    "        weights3 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3],\n",
    "                               stddev=np.sqrt(2.0/num_hidden_nodes2))\n",
    "            )\n",
    "        biases3  = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "        # layer4\n",
    "        weights4 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden_nodes3, num_labels],\n",
    "                               stddev=np.sqrt(2.0/num_hidden_nodes3))\n",
    "            )\n",
    "        biases4  = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "        # Training computation - Layer1\n",
    "        layer1_out = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "        drop1      = tf.nn.dropout(layer1_out, keep_prob)   # Dropout 1\n",
    "        # Layer 2\n",
    "        layer2_out = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "        drop2      = tf.nn.dropout(layer2_out, keep_prob)   # Dropout 2\n",
    "        # Layer 3\n",
    "        layer3_out = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "        drop3      = tf.nn.dropout(layer3_out, keep_prob)   # Dropout 2\n",
    "        # Logits \n",
    "        logits = tf.matmul(drop3, weights4) + biases4\n",
    "        # Loss\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "            beta * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4))\n",
    "        \n",
    "        # Optimizer, with Learning Rate schedule\n",
    "        learning_rate = tf.train.exponential_decay(0.5, global_step, 2000, 0.7, staircase=False)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "        # Preductions for training / validation / test\n",
    "        train_prediction  = tf.nn.softmax(logits)\n",
    "        layer1_validation = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "        layer2_validation = tf.nn.relu(tf.matmul(layer1_validation, weights2) + biases2)\n",
    "        layer3_validation = tf.nn.relu(tf.matmul(layer2_validation, weights3) + biases3)\n",
    "        valid_prediction  = tf.nn.softmax(tf.matmul(layer3_validation, weights4) + biases4)\n",
    "        \n",
    "        layer1_test       = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "        layer2_test       = tf.nn.relu(tf.matmul(layer1_test, weights2) + biases2)\n",
    "        layer3_test       = tf.nn.relu(tf.matmul(layer2_test, weights3) + biases3)\n",
    "        test_prediction   = tf.nn.softmax(tf.matmul(layer3_test, weights4) + biases4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized, with reg: 0.001\n",
      "minibatch loss at step 0: 4.007266\n",
      "minibatch accuracy : 9.375\n",
      "validation accuracy: 15.58\n",
      "minibatch loss at step 500: 1.385044\n",
      "minibatch accuracy : 85.9375\n",
      "validation accuracy: 83.95\n",
      "minibatch loss at step 1000: 1.159193\n",
      "minibatch accuracy : 85.1562\n",
      "validation accuracy: 84.91\n",
      "minibatch loss at step 1500: 0.814913\n",
      "minibatch accuracy : 88.2812\n",
      "validation accuracy: 85.53\n",
      "minibatch loss at step 2000: 0.694715\n",
      "minibatch accuracy : 91.4062\n",
      "validation accuracy: 85.64\n",
      "minibatch loss at step 2500: 0.679360\n",
      "minibatch accuracy : 86.7188\n",
      "validation accuracy: 86.5\n",
      "minibatch loss at step 3000: 0.749631\n",
      "minibatch accuracy : 85.9375\n",
      "validation accuracy: 85.91\n",
      "minibatch loss at step 3500: 0.733149\n",
      "minibatch accuracy : 82.0312\n",
      "validation accuracy: 86.51\n",
      "minibatch loss at step 4000: 0.630454\n",
      "minibatch accuracy : 88.2812\n",
      "validation accuracy: 86.51\n",
      "minibatch loss at step 4500: 0.604914\n",
      "minibatch accuracy : 87.5\n",
      "validation accuracy: 86.44\n",
      "minibatch loss at step 5000: 0.650062\n",
      "minibatch accuracy : 87.5\n",
      "validation accuracy: 86.83\n",
      "minibatch loss at step 5500: 0.725694\n",
      "minibatch accuracy : 82.8125\n",
      "validation accuracy: 86.47\n",
      "minibatch loss at step 6000: 0.694435\n",
      "minibatch accuracy : 84.375\n",
      "validation accuracy: 86.8\n",
      "minibatch loss at step 6500: 0.573638\n",
      "minibatch accuracy : 89.0625\n",
      "validation accuracy: 86.99\n",
      "minibatch loss at step 7000: 0.698751\n",
      "minibatch accuracy : 83.5938\n",
      "validation accuracy: 87.24\n",
      "minibatch loss at step 7500: 0.809067\n",
      "minibatch accuracy : 81.25\n",
      "validation accuracy: 87.09\n",
      "minibatch loss at step 8000: 0.970625\n",
      "minibatch accuracy : 77.3438\n",
      "validation accuracy: 86.62\n",
      "minibatch loss at step 8500: 0.586962\n",
      "minibatch accuracy : 89.8438\n",
      "validation accuracy: 86.87\n",
      "minibatch loss at step 9000: 0.689502\n",
      "minibatch accuracy : 86.7188\n",
      "validation accuracy: 87.04\n",
      "minibatch loss at step 9500: 0.726133\n",
      "minibatch accuracy : 84.375\n",
      "validation accuracy: 87.23\n",
      "minibatch loss at step 10000: 0.698362\n",
      "minibatch accuracy : 82.0312\n",
      "validation accuracy: 87.42\n",
      "\n",
      "Test accuracy: 93.48\n"
     ]
    }
   ],
   "source": [
    "## Train and run the network\n",
    "num_steps = 10001\n",
    "\n",
    "beta_fixed = 1e-3   # Testing with one value of Regularization param\n",
    "keep_prob_val = 0.5 # Dropout percentage\n",
    "\n",
    "step_vals = []   # step number store\n",
    "test_acc_vals = []  # Test accuracy store\n",
    "val_acc_vals  = []  # Validation acc store\n",
    "\n",
    "## For limiting GPU memory usage: \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized, with reg:\", beta_fixed)\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a mini batch\n",
    "        batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "        # Make a dict telling the session where to find the minibatch\n",
    "        feed_dict = { tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: beta_fixed,            # Regul param\n",
    "                     keep_prob: keep_prob_val}    # Dropout from above\n",
    "        # Run the damn thing\n",
    "        _, loss_step, predictions = session.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        # show progress  \n",
    "        if (step % 500 == 0):\n",
    "            print(\"minibatch loss at step %d: %f\" % (step, loss_step))\n",
    "            val_acc  = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            test_acc = accuracy(test_prediction.eval(), test_labels)\n",
    "            print(\"minibatch accuracy : %g\" % accuracy(predictions, batch_labels))\n",
    "            print(\"validation accuracy: %g\" % val_acc)\n",
    "            step_vals.append(step)\n",
    "            val_acc_vals.append(val_acc)\n",
    "            test_acc_vals.append(test_acc)\n",
    "            \n",
    "    print(\"\\nTest accuracy: %g\" % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137794f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try plotting\n",
    "plt.plot(step_vals, test_acc_vals, '-')\n",
    "plt.plot(step_vals, val_acc_vals, '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max test:  93.97\n"
     ]
    }
   ],
   "source": [
    "m = myargmax(test_acc_vals)\n",
    "print('max test: ', test_acc_vals[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
