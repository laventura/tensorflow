{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recurrent Neural Network - with Keras\n",
    "\n",
    "Implementing char-RNN with Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import random \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up input text data. in this case - these are SOTU (State of the Union) speeches \n",
    "# by US Presidents\n",
    "text_files = glob('data/sotu/*.txt')\n",
    "text = '\\n'.join([open(f, 'r').read() for f in text_files])\n",
    "\n",
    "# get all (unique) chars - these are our 'categories' or 'labels'\n",
    "chars = list(set(text))\n",
    "\n",
    "# set a fixed vector size\n",
    "# so we look at specific window size\n",
    "max_len = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2942683\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "# see how much data\n",
    "\n",
    "LEN_TEXT = len(text)\n",
    "NUM_LABELS = len(chars)\n",
    "\n",
    "print(LEN_TEXT)\n",
    "print(NUM_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is classification. Given a sequence of chars, we will predict the next character, based on its probability. \n",
    "Each character in the vocabulary has a label, e.g. \"a\" is 0, \"b\" is 1, etc.\n",
    "\n",
    "We use _softmax_ activation (used for categorical ouput) on the output layer to generate probabilities for the predicted character. The char with the highest probability is our best guess. \n",
    "\n",
    "The _categorical crossentropy_ loss function is standard for multiple classification; it essentially penalizes the network more the further off it is from the correct label.\n",
    "\n",
    "We use _dropout_ to prevent overfitting. We don't want the network to memorize everything, we want some novelty; dropout prevents the network from overfitting. We use anywhere from 20% to 50% dropout (meaning the network with turn off those nodes/neurons).\n",
    "\n",
    "To train, we chunk the training data into chunks of size `max_len`. We match the chunks with the char that immediately follows each sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define our RNN model - to predict the next single character\n",
    "model = Sequential()\n",
    "# 1st layer: LSTM of 512 nodes\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(max_len, NUM_LABELS) ))\n",
    "model.add(Dropout(0.25))\n",
    "# 2nd LSTM layer\n",
    "model.add(LSTM(256, return_sequences=False))\n",
    "model.add(Dropout(0.25))\n",
    "# flatten out - \n",
    "model.add(Dense(NUM_LABELS))\n",
    "# last one - softmax activation\n",
    "model.add(Activation('softmax'))\n",
    "# now compile the mode\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mind which has b\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "# Let's try some sample text\n",
    "# Sample quote from Bertrand Russell\n",
    "example_text = \"The mind which has become accustomed to the freedom and impartiality of philosophic contemplation will preserve something of the same freedom and impartiality in the world of action and emotion. It will view its purposes and desires as parts of the whole, with the absence of insistence that results from seeing them as infinitesimal fragments in a world of which all the rest is unaffected by any one man's deeds. The impartiality which, in contemplation, is the unalloyed desire for truth, is the very same quality of mind which, in action, is justice, and in emotion is that universal love which can be given to all, and not only to those who are judged useful or admirable. Thus contemplation enlarges not only the objects of our thoughts, but also the objects of our actions and our affections: it makes us citizens of the universe, not only of one walled city at war with all the rest. In this citizenship of the universe consists man's true freedom, and his liberation from the thraldom of narrow hopes and fears.\"\n",
    "\n",
    "# step size of 3\n",
    "input_1 = example_text[0:20]\n",
    "true_out1 = example_text[20]\n",
    "print(input_1)\n",
    "print(true_out1)\n",
    "\n",
    "input_2 = example_text[3:23]\n",
    "true_out2 = example_text[23]\n",
    "\n",
    "input_3 = example_text[6:26]\n",
    "true_out2 = example_text[26]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980888\n",
      "980888\n"
     ]
    }
   ],
   "source": [
    "step = 3\n",
    "inputs  = []\n",
    "outputs = []\n",
    "for i in range(0, len(text) - max_len, step):\n",
    "    inputs.append (text[i: i+max_len])\n",
    "    outputs.append(text[i+max_len])\n",
    "    \n",
    "print(len(inputs))\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map each char to its label and vice versa. This is also called the encoder / decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Encoder and Decoder: map from chars to label and reverse\n",
    "char2labels = {ch: i for i, ch in enumerate(chars)}\n",
    "labels2char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 40, '-': 0, 's': 1, '\\n': 69, '+': 3, '3': 41, 'g': 42, 'q': 4, '%': 43, 'h': 5, 'N': 44, 'B': 6, 'R': 7, 'p': 8, ']': 9, \"'\": 46, '7': 2, 'V': 10, 'A': 47, 'l': 48, 'v': 49, 'z': 12, 'o': 50, 'd': 13, '\\x95': 52, '1': 15, 'J': 53, '$': 54, 'Q': 16, 't': 56, 'H': 55, 'M': 68, 'Y': 57, 'e': 17, 'D': 60, 'n': 61, 'O': 18, '½': 19, '(': 86, ';': 20, 'f': 62, 'w': 11, 'S': 51, 'T': 21, '?': 63, ' ': 64, '!': 65, 'k': 66, 'W': 67, '5': 22, 'C': 14, '`': 23, 'U': 24, '6': 36, 'c': 70, ':': 71, '¼': 72, 'X': 25, '8': 26, '0': 73, ')': 27, '.': 74, 'b': 75, '[': 76, '4': 28, '¢': 77, '&': 29, 'L': 78, ',': 30, 'K': 39, 'a': 31, 'F': 32, '/': 33, 'u': 79, 'E': 34, 'i': 35, 'G': 80, '2': 58, 'y': 81, 'j': 82, 'm': 83, 'Z': 37, 'r': 38, '9': 84, 'P': 85, '\"': 45, '*': 59, 'I': 87, '_': 88}\n"
     ]
    }
   ],
   "source": [
    "# quick test to see what the dicts contain\n",
    "print(char2labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980888, 20, 89)\n",
      "(980888, 89)\n"
     ]
    }
   ],
   "source": [
    "# define X input and Y output label Tensors\n",
    "# use book to reduce memory usage\n",
    "\n",
    "# X shape: depth x max_len x num_labels\n",
    "X = np.zeros( (len(inputs), max_len, NUM_LABELS), dtype=np.bool) \n",
    "# y shape: depth x NUM_LABELS\n",
    "y = np.zeros( (len(inputs), NUM_LABELS), dtype=np.bool )  \n",
    "\n",
    "# set appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char2labels[char]] = 1\n",
    "    y[i, char2labels[outputs[i]]]  = 1\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now start training \n",
    "epochs = 2    # can be 10 or higher, but will need a GPU to hasten it\n",
    "model.fit(X, y, \n",
    "         batch_size=128,\n",
    "         nb_epoch=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's write a generator func that will now generate chars based on the network's predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _temperature_ controls how random we want the network to be. A lower temp means favoring more likely values; a higher temp means more randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(temperature=0.3, seed=None, predicate=lambda x: len(x) < 100):\n",
    "    \"\"\"  Returns a new generated sentence (of upto 100 chars)\n",
    "    \"\"\"\n",
    "    if seed is not None and len(seed) < max_len:\n",
    "        raise Exception('seed must be atleast {} chars long'.format(max_len))\n",
    "    \n",
    "    # if no seed text, use random\n",
    "    else: \n",
    "        start_ix = random.randint(0, len(text) - max_len - 1)\n",
    "        seed = text[start_ix: start_ix + max_len]\n",
    "    \n",
    "    sentence = seed\n",
    "    generated = sentence\n",
    "    \n",
    "    while predicate(generated):\n",
    "        # create input tensor\n",
    "        # from the last max_len chars generated so far\n",
    "        x = np.zeros( (1, max_len, len(chars)) )\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char2labels[char]] = 1.\n",
    "        \n",
    "        # produce a prob distribution over the chars\n",
    "        probs = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # sample the character to use based on predicted probabilities\n",
    "        next_idx  = sample(probs, temperature)\n",
    "        next_char = labels2char[next_idx]\n",
    "        \n",
    "        generated  += next_char\n",
    "        sentence    = sentence[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "def sample(probabilities, temperature):\n",
    "    \"\"\" samples an index from a vecor of probabilities\n",
    "    \"\"\"\n",
    "    a = np.log(probabilities)/temperature\n",
    "    distr = np.exp(a)/np.sum(np.exp(a))\n",
    "    choices = range(len(probabilities))\n",
    "    return np.random.choice(choices, p=distr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some sample text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Epoch 1/1\n",
      "980888/980888 [==============================] - 3050s - loss: 1.8025  \n",
      "temperature: 0.33\n",
      "p the sign that says to the for the consideration to the war in the last year to the Congress of the\n",
      "temperature: 0.66\n",
      "urgent and intense. The $ay to all to the Union in the people the for refigred and the finst the Nea\n",
      "temperature: 1.00\n",
      "s State lines, and in we can be increase person schools. As their over I inough the duscork or. Do a\n",
      "epoch 1\n",
      "Epoch 1/1\n",
      "980888/980888 [==============================] - 3077s - loss: 1.4197  \n",
      "temperature: 0.33\n",
      " quarter of their interest of the restore the people and the security to the community and the prive\n",
      "temperature: 0.66\n",
      "e of the Nation's recommends that the abort of the country college is a new source of the Soviet cha\n",
      "temperature: 1.00\n",
      "tatorship to take over, path America that to nection is over mall and stade and commanded drefice. \n",
      "\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "167808/980888 [====>.........................] - ETA: 2518s - loss: 1.3544"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7336237b5b34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# set nb_epoch to 1 since we iterating manually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# comment to just generate text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# preview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1104\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    822\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 4   # Can be 10.. but need more horse power\n",
    "for i in range(epochs):\n",
    "    print('epoch %d'%i)\n",
    "    \n",
    "    # set nb_epoch to 1 since we iterating manually\n",
    "    # comment to just generate text\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "    \n",
    "    # preview\n",
    "    for temp in [0.33, 0.66, 1.0]:\n",
    "        print('temperature: %0.2f' % temp)\n",
    "        print('%s' % generate(temperature=temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
