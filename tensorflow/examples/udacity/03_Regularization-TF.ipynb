{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Assignment 3\n",
    "Previously in 2_fullyconnected.ipynb, you trained a logistic regression and a neural network model.\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define a measure of Accuracy for Predictions and Labels\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "sd = 1.0 / np.sqrt(batch_size)\n",
    "\n",
    "num_hidden_units1 = 1024\n",
    "num_hidden_units2 = 512\n",
    "learning_rate = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/cpu:0'):   # Forcing on CPU (as TF is complaining of GPU out of mem)\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        #X = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size)) # Features\n",
    "        #Y = tf.placeholder(tf.float32, shape=(batch_size, num_labels))   # labels\n",
    "        \n",
    "        tf_train_dataset = tf.placeholder( tf.float32, \n",
    "                                         shape=(batch_size, image_size*image_size))\n",
    "        tf_train_labels  = tf.placeholder( tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset  = tf.constant(test_dataset)\n",
    "        \n",
    "        beta = tf.placeholder(tf.float32)   # param for Regularization\n",
    "        keep_prob = tf.placeholder(tf.float32)   # for drop out\n",
    "        \n",
    "        # Variables\n",
    "        W1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_units1], \n",
    "                                            mean=0, stddev=np.sqrt(2.0/(image_size*image_size))))\n",
    "        b1 = tf.Variable(tf.zeros([num_hidden_units1]))\n",
    "        \n",
    "        W2 = tf.Variable(tf.truncated_normal([num_hidden_units1, num_hidden_units2], \n",
    "                                            mean=0, stddev=np.sqrt(2.0/num_hidden_units1)))\n",
    "        b2 = tf.Variable(tf.zeros([num_hidden_units2]))\n",
    "        \n",
    "        # final weights\n",
    "        W  = tf.Variable(tf.truncated_normal([num_hidden_units2, num_labels], \n",
    "                                            mean=0, stddev=np.sqrt(2.0/num_hidden_units2)))\n",
    "        b  = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "        def model(data):  \n",
    "            \"\"\" Assembles the model, using given dataset\n",
    "            Return h = X . W + b\n",
    "            \"\"\"\n",
    "            h1 = tf.nn.relu(tf.matmul(data,  W1) + b1)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob)     # Dropout\n",
    "            h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "            #h2 = tf.nn.dropout(h2, keep_prob)      # Dropout\n",
    "            return tf.matmul(h2, W) + b\n",
    "        \n",
    "        \n",
    "        logits = model(tf_train_dataset)    # for Training dataset\n",
    "        #y_     = tf.nn.softmax(logits)   # scaled logits, after softmax\n",
    "        \n",
    "        # Loss calc - on Training\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "                 beta * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "        # predictions\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))  # eval on Validation\n",
    "        test_prediction  = tf.nn.softmax(model(tf_test_dataset))   # eval on Test data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "minibatch loss at step 0: 3.659582\n",
      "minibatch acc : 7.0%\n",
      "validation acc: 27.0%\n",
      "minibatch loss at step 500: 1.177359\n",
      "minibatch acc : 90.6%\n",
      "validation acc: 85.1%\n",
      "minibatch loss at step 1000: 1.056028\n",
      "minibatch acc : 82.8%\n",
      "validation acc: 85.5%\n",
      "minibatch loss at step 1500: 0.673375\n",
      "minibatch acc : 89.8%\n",
      "validation acc: 86.8%\n",
      "minibatch loss at step 2000: 0.593424\n",
      "minibatch acc : 92.2%\n",
      "validation acc: 87.3%\n",
      "minibatch loss at step 2500: 0.624670\n",
      "minibatch acc : 87.5%\n",
      "validation acc: 86.9%\n",
      "minibatch loss at step 3000: 0.645932\n",
      "minibatch acc : 84.4%\n",
      "validation acc: 87.4%\n",
      "minibatch loss at step 3500: 0.656286\n",
      "minibatch acc : 86.7%\n",
      "validation acc: 87.4%\n",
      "minibatch loss at step 4000: 0.501366\n",
      "minibatch acc : 87.5%\n",
      "validation acc: 87.6%\n",
      "minibatch loss at step 4500: 0.503272\n",
      "minibatch acc : 89.1%\n",
      "validation acc: 87.1%\n",
      "minibatch loss at step 5000: 0.545159\n",
      "minibatch acc : 88.3%\n",
      "validation acc: 87.7%\n",
      "Test accuracy : 93.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "beta_fixed = 1e-3\n",
    "keep_prob_train = 0.5\n",
    "keep_prob_test = 1.0\n",
    "\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "y_true, y_pred = None, None\n",
    "\n",
    "## For limiting GPU memory usage: \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a mini batch\n",
    "        batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "        # Make a dict telling the session where to find the minibatch\n",
    "        feed_dict = {tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: beta_fixed,    \n",
    "                     keep_prob: keep_prob_train}    \n",
    "        # run one step\n",
    "        _, step_loss, predictions = session.run([optimizer, loss, train_prediction], \n",
    "                                        feed_dict=feed_dict)\n",
    "        cost_history = np.append(cost_history, step_loss)   # save the loss\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print (\"minibatch loss at step %d: %f\" % (step, step_loss))\n",
    "            print (\"minibatch acc : %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print (\"validation acc: %.1f%%\" % accuracy(valid_prediction.eval(feed_dict={\n",
    "                                                                 keep_prob: keep_prob_test}), \n",
    "                                                valid_labels))\n",
    "    \n",
    "    print (\"Test accuracy : %.1f%%\" % accuracy(test_prediction.eval(feed_dict={\n",
    "                                                                   keep_prob: keep_prob_test}),\n",
    "                                                test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trying with Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "sd = 1.0 / np.sqrt(batch_size)\n",
    "\n",
    "num_hidden_units1 = 1024\n",
    "num_hidden_units2 = 512\n",
    "learning_rate = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/cpu:0'):   # Forcing on CPU (as TF is complaining of GPU out of mem)\n",
    "        # Input data. For the training data, we use a placeholder that will be fed\n",
    "        # at run time with a training minibatch.\n",
    "        #X = tf.placeholder(tf.float32, shape=(batch_size, image_size*image_size)) # Features\n",
    "        #Y = tf.placeholder(tf.float32, shape=(batch_size, num_labels))   # labels\n",
    "        \n",
    "        tf_train_dataset = tf.placeholder( tf.float32, \n",
    "                                         shape=(batch_size, image_size*image_size))\n",
    "        tf_train_labels  = tf.placeholder( tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset  = tf.constant(test_dataset)\n",
    "        \n",
    "        beta = tf.placeholder(tf.float32)   # param for Regularization\n",
    "        keep_prob = tf.placeholder(tf.float32)   # for drop out\n",
    "        global_step = tf.Variable(0)\n",
    "        \n",
    "        \n",
    "        # Variables\n",
    "        W1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_units1], \n",
    "                                            mean=0, stddev=np.sqrt(2.0/(image_size*image_size))))\n",
    "        b1 = tf.Variable(tf.zeros([num_hidden_units1]))\n",
    "        \n",
    "        W2 = tf.Variable(tf.truncated_normal([num_hidden_units1, num_hidden_units2], \n",
    "                                            mean=0, stddev=np.sqrt(2.0/num_hidden_units1)))\n",
    "        b2 = tf.Variable(tf.zeros([num_hidden_units2]))\n",
    "        \n",
    "        # final weights\n",
    "        W  = tf.Variable(tf.truncated_normal([num_hidden_units2, num_labels], \n",
    "                                            mean=0, stddev=np.sqrt(2.0/num_hidden_units2)))\n",
    "        b  = tf.Variable(tf.zeros([num_labels]))\n",
    "        \n",
    "        def model(data):  \n",
    "            \"\"\" Assembles the model, using given dataset\n",
    "            Return h = X . W + b\n",
    "            \"\"\"\n",
    "            h1 = tf.nn.relu(tf.matmul(data,  W1) + b1)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob)     # Dropout\n",
    "            h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "            #h2 = tf.nn.dropout(h2, keep_prob)      # Dropout\n",
    "            return tf.matmul(h2, W) + b\n",
    "        \n",
    "        \n",
    "        logits = model(tf_train_dataset)    # for Training dataset\n",
    "        #y_     = tf.nn.softmax(logits)   # scaled logits, after softmax\n",
    "        \n",
    "        # Loss calc - on Training\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "                 beta * (tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W))\n",
    "            \n",
    "        learning_rate = tf.train.exponential_decay(0.5, global_step, 2000, 0.65, staircase=True)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "        \n",
    "        # predictions\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        valid_prediction = tf.nn.softmax(model(tf_valid_dataset))  # eval on Validation\n",
    "        test_prediction  = tf.nn.softmax(model(tf_test_dataset))   # eval on Test data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "minibatch loss at step 0: 3.599866\n",
      "minibatch acc : 9.4%\n",
      "validation acc: 23.2%\n",
      "minibatch loss at step 500: 1.179434\n",
      "minibatch acc : 89.8%\n",
      "validation acc: 84.9%\n",
      "minibatch loss at step 1000: 1.066988\n",
      "minibatch acc : 82.8%\n",
      "validation acc: 86.0%\n",
      "minibatch loss at step 1500: 0.689836\n",
      "minibatch acc : 90.6%\n",
      "validation acc: 86.7%\n",
      "minibatch loss at step 2000: 0.595683\n",
      "minibatch acc : 92.2%\n",
      "validation acc: 87.3%\n",
      "minibatch loss at step 2500: 0.583258\n",
      "minibatch acc : 88.3%\n",
      "validation acc: 87.5%\n",
      "minibatch loss at step 3000: 0.617804\n",
      "minibatch acc : 85.9%\n",
      "validation acc: 87.6%\n",
      "minibatch loss at step 3500: 0.656064\n",
      "minibatch acc : 82.8%\n",
      "validation acc: 88.0%\n",
      "minibatch loss at step 4000: 0.513816\n",
      "minibatch acc : 91.4%\n",
      "validation acc: 88.2%\n",
      "minibatch loss at step 4500: 0.486565\n",
      "minibatch acc : 87.5%\n",
      "validation acc: 88.1%\n",
      "minibatch loss at step 5000: 0.524385\n",
      "minibatch acc : 90.6%\n",
      "validation acc: 88.8%\n",
      "minibatch loss at step 5500: 0.558334\n",
      "minibatch acc : 87.5%\n",
      "validation acc: 88.6%\n",
      "minibatch loss at step 6000: 0.598413\n",
      "minibatch acc : 85.2%\n",
      "validation acc: 88.4%\n",
      "minibatch loss at step 6500: 0.444616\n",
      "minibatch acc : 91.4%\n",
      "validation acc: 89.0%\n",
      "minibatch loss at step 7000: 0.542811\n",
      "minibatch acc : 86.7%\n",
      "validation acc: 89.0%\n",
      "minibatch loss at step 7500: 0.585276\n",
      "minibatch acc : 85.9%\n",
      "validation acc: 89.1%\n",
      "minibatch loss at step 8000: 0.650028\n",
      "minibatch acc : 82.8%\n",
      "validation acc: 89.0%\n",
      "Test accuracy : 94.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 8001\n",
    "beta_fixed = 1e-3\n",
    "keep_prob_train = 0.5\n",
    "keep_prob_test = 1.0\n",
    "\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "y_true, y_pred = None, None\n",
    "\n",
    "## For limiting GPU memory usage: \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a mini batch\n",
    "        batch_data   = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels [offset:(offset + batch_size), :]\n",
    "        # Make a dict telling the session where to find the minibatch\n",
    "        feed_dict = {tf_train_dataset: batch_data,\n",
    "                     tf_train_labels: batch_labels,\n",
    "                     beta: beta_fixed,    \n",
    "                     keep_prob: keep_prob_train}    \n",
    "        # run one step\n",
    "        _, step_loss, predictions = session.run([optimizer, loss, train_prediction], \n",
    "                                        feed_dict=feed_dict)\n",
    "        cost_history = np.append(cost_history, step_loss)   # save the loss\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print (\"minibatch loss at step %d: %f\" % (step, step_loss))\n",
    "            print (\"minibatch acc : %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print (\"validation acc: %.1f%%\" % accuracy(valid_prediction.eval(feed_dict={\n",
    "                                                                 keep_prob: keep_prob_test}), \n",
    "                                                valid_labels))\n",
    "    \n",
    "    print (\"Test accuracy : %.1f%%\" % accuracy(test_prediction.eval(feed_dict={\n",
    "                                                                   keep_prob: keep_prob_test}),\n",
    "                                                test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAHpCAYAAAAhyVBgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12e7e45c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(cost_history)\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.axis([0,num_steps,0,np.max(cost_history)])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
