{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary association of autonomous individuals \n",
      "1000  anarchism originated as a term of abuse first used against early working class radicals including t\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "\n",
    "print(train_size, train_text[:100])\n",
    "print(valid_size, valid_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "NUM_NODES = num_nodes = 64\n",
    "VOCAB_SIZE = vocabulary_size\n",
    "BATCH_SIZE = batch_size\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "  saved_state  = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([NUM_NODES, VOCAB_SIZE], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"\n",
    "    Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\n",
    "    Returns output (calculated) and current state\n",
    "    \"\"\"\n",
    "    input_gate  = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update      = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[BATCH_SIZE,VOCAB_SIZE]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output  = saved_output\n",
    "  state   = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss   = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, \n",
    "                                                                    tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, \n",
    "                                             global_step, \n",
    "                                             5000, \n",
    "                                             0.1, \n",
    "                                             staircase=True)\n",
    "  optimizer    = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)   # gradient clipping to prevent exploding\n",
    "  optimizer    = optimizer.apply_gradients(zip(gradients, v), \n",
    "                                        global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input        = tf.placeholder(tf.float32, shape=[1, VOCAB_SIZE])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  saved_sample_state  = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  reset_sample_state  = tf.group(saved_sample_output.assign(tf.zeros([1, NUM_NODES])),\n",
    "                                 saved_sample_state.assign (tf.zeros([1, NUM_NODES])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, \n",
    "                                          saved_sample_output, \n",
    "                                          saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293939 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "troisjsnnccu eslfijestvjbpaarlymytvnoot ueeu sls rwt eimtik ejh jtdephy pe tcfnu\n",
      " efusjnigvdg jisjtscktjqkcjyvv j dkwuid mri un   hyteedtyehq glfgg mhuvd hdduxq \n",
      "im  esdfaobuc hbttu fmhlierjwnan udht xcgoawfgb bqje hloluoacattqi ati  pnyrvtia\n",
      "gy  hmee  sum eoyov vg etll oagdzhlkamuhy gxiphxe  gwrciicdcge  ozzt zrqv sywt v\n",
      "ut oi hqsesjtwy acy   kythcn  ql t wlh  b iekue v gpfgw tapxerlxyjgstf isvempfh \n",
      "================================================================================\n",
      "Validation set perplexity: 20.10\n",
      "Average loss at step 100: 2.589627 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.89\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200: 2.252437 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.66\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 300: 2.100285 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 400: 2.002610 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.82\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 500: 1.932163 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 600: 1.906197 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 700: 1.858041 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.817394 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 900: 1.828313 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1000: 1.821694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "ble in they muccorn of the wearerronome coxpetical lithy to the cell the is it i\n",
      "f of and can in cere oarbiaity theie ta that move opernes the sone of generou of\n",
      "getabllo the eovel a leation prijstted not encus in the ured are in these to fin\n",
      " how zer whe bost be and as precesang gixp conttove atteen the secime covennate \n",
      "re reever it or for airstigually he siffict kight fion zero two zero aymounic re\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.776508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1200: 1.752025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1300: 1.734051 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1400: 1.744949 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1500: 1.736544 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1600: 1.745851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1700: 1.711388 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1800: 1.674134 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.650366 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2000: 1.701070 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "que in exter a laft liptogy s shaatts in ted madwhy ghowop cussing of dren what \n",
      "e in the felon eastaming ro otter and troon havimat ablows aboocla commother reg\n",
      "zerncipart hel atic of a scost ipperting but whice occoubing progun coust reton \n",
      "ptylers duen profedenon wor his olick calling usi fom thout molowd produpial roc\n",
      "x digue duriasion of the comms camper hass his wod zer two was tyrogry a volaced\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2100: 1.689172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2200: 1.684037 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2300: 1.638082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2400: 1.660315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2500: 1.679821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.652562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2700: 1.655168 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2800: 1.649385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2900: 1.649576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3000: 1.649725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "zeralt s marrio last boldan bould the elose vidion aytronfa sedelating pastix l \n",
      "quirate and and final dibbolt the create in cilly eight syetsine rychigle of gov\n",
      "gently trade as dismantbimia the peac off re lackel mperifally anaschetly may wh\n",
      "ous evolusing pollona in fataan spection filed to atoras se so projiksmantagesce\n",
      "wate piay sways in caxjolore a presorving and papes oftt in the amexgment lindon\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3100: 1.628229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3200: 1.645190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3300: 1.639949 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3400: 1.668232 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3500: 1.655652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.668658 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3700: 1.646776 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.643745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3900: 1.634696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4000: 1.652247 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "================================================================================\n",
      "y attingpanied notal incledrige or terrotill slaselga seated for aparmated some \n",
      "ate they chumb butlizos whom with fanse within expenncis and mprimational primit\n",
      "um some especial dirb ezerbiry strixtoformate extme stazzed he in represed the r\n",
      "rimantso have genenatuita cull dealiaman posing one it dumus the everants which \n",
      "kry one nine seven seven five methoroped toracite dd hindon useulian he somely p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4100: 1.634899 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4200: 1.634272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.614123 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4400: 1.607946 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4500: 1.613570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4600: 1.614876 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4700: 1.627028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4800: 1.630280 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4900: 1.633184 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5000: 1.608833 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "zine most a controos less this six one eaging one one six five four who zero wor\n",
      "firm woodent pater is rages will in and blynks descretic one six julsion avown m\n",
      "ited that held is noting one zero s accies informeers jurities s trean georgu in\n",
      "old is before leiftered excreated two five zero three zero zero had elmove low d\n",
      "pe were germany of jutt that three one nine four five eight three five one nine \n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5100: 1.605512 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5200: 1.594016 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5300: 1.582175 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5400: 1.582261 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5500: 1.574547 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5600: 1.587320 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.572410 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.579492 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5900: 1.577782 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6000: 1.551564 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "maked will anements is of corniog e y class it eight nine three one due number o\n",
      "red were jeakingsomon the old indic been all add having the of frinter military \n",
      "zie golder and a wean all ell of one nine one nine nine one nine four five five \n",
      "zever stooked on these they one nine nine rogue on the confebrate cinter a north\n",
      "x burnon one nine eight three one nine five the viljo action dotensed ray did th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6100: 1.568139 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.537735 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6300: 1.548060 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6400: 1.547756 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6500: 1.561024 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6600: 1.600305 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6700: 1.587088 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6800: 1.607727 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.586488 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7000: 1.583503 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "tainzett candomat zego half has dvstificmediatuan and revolute as war the westin\n",
      "jequiin of the kogned ten thang and swident of new were believe share ibry ameri\n",
      "fored with enen friended lick note kensions namud s blats is two three or astril\n",
      "ment addicity bay computer it but during are of diponotion to one one nine nine \n",
      "ord kethorenture of mee and commutical occastroce penishal an ectnomical many ma\n",
      "================================================================================\n",
      "Validation set perplexity: 4.27\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    # create feed \n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    # run one step \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], \n",
    "                                        feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' \n",
    "            % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' \n",
    "            % float(np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "      # Periodically generate some samples\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed     = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed       = sample(prediction)\n",
    "            sentence  += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "    \n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions   = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' \n",
    "            % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_NODES = num_nodes = 64\n",
    "VOCAB_SIZE = vocabulary_size\n",
    "BATCH_SIZE = batch_size\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  \n",
    "  # combine all vars together (to reduce matrix multiplications)\n",
    "  px = tf.concat(1, [ix, fx, cx, ox])   # Input, Forget, (Memory) Update, Ouput\n",
    "  pm = tf.concat(1, [im, fm, cm, om])\n",
    "  pb = tf.concat(1, [ib, fb, cb, ob])\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "  saved_state  = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([NUM_NODES, VOCAB_SIZE], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"\n",
    "    Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\n",
    "    Returns output (calculated) and current state\n",
    "    \"\"\"\n",
    "    # combined matrix multiplication\n",
    "    comb_matmul = tf.matmul(i, px) + tf.matmul(o, pm) + pb\n",
    "    # now split the large matrix into four (clever, but not that clever)\n",
    "    inp_mat, forget_mat, update, outp_mat = tf.split(1, 4, comb_matmul)\n",
    "    input_gate    = tf.sigmoid(inp_mat)\n",
    "    forget_gate   = tf.sigmoid(forget_mat)\n",
    "    output_gate   = tf.sigmoid(outp_mat)\n",
    "    state         = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    # input_gate  = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    # update      = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    # state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, \n",
    "                                     shape=[BATCH_SIZE,VOCAB_SIZE]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output  = saved_output\n",
    "  state   = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss   = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, \n",
    "                                                                    tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, \n",
    "                                             global_step, \n",
    "                                             5000, \n",
    "                                             0.1, \n",
    "                                             staircase=True)\n",
    "  optimizer    = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)   # gradient clipping to prevent exploding\n",
    "  optimizer    = optimizer.apply_gradients(zip(gradients, v), \n",
    "                                        global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input        = tf.placeholder(tf.float32, shape=[1, VOCAB_SIZE])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  saved_sample_state  = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  reset_sample_state  = tf.group(saved_sample_output.assign(tf.zeros([1, NUM_NODES])),\n",
    "                                 saved_sample_state.assign (tf.zeros([1, NUM_NODES])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, \n",
    "                                          saved_sample_output, \n",
    "                                          saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292014 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.90\n",
      "================================================================================\n",
      "  mic e dreka svez rxonh ozycsiwbm antrhlthcmul uyskaifudkrzigeuedsharsegezpa lp\n",
      "e pi  t zplvbetaevosz amaizrehzukguao th alvehuot izgryxr svhirdewhhnrfovnxbrsle\n",
      "ce m agtaycntiveo frh cezf tdamukz smrooo afmntkevhtra rizphnrrgbkh irha dnrl vh\n",
      "fengghiohkode ydtzentbgipgtaluimunzvc jxnfwsfio   kaxdakh yc mih kvmuas  xt blli\n",
      "htrltoeu armx d xjgvrvlexzhgamirqthc ahevitujf  eqz nkaepodavh eslasplwj tthzzh \n",
      "================================================================================\n",
      "Validation set perplexity: 20.04\n",
      "Average loss at step 100: 2.583414 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.22\n",
      "Validation set perplexity: 12.08\n",
      "Average loss at step 200: 2.250357 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.28\n",
      "Validation set perplexity: 8.78\n",
      "Average loss at step 300: 2.085532 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 400: 2.036052 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.71\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 500: 1.984873 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 600: 1.901149 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.873629 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 800: 1.874460 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 900: 1.849122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 1000: 1.848095 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "================================================================================\n",
      "quad wime lowe the with eight four lave easkd a numasic countraus in hradgiel s \n",
      "y try curror on yewerse mansi sumbisal trawel plicaties of than bemadian leaj fo\n",
      "plabed san pust wih one is mationing buinkibh in plisent thoirare ronlitian of c\n",
      "que for javed glofing prekn to caterbins auter spageatic is consado be the schin\n",
      "y the be bill builts b ys extring c hast secatide bromly cordiccressh gust d ys \n",
      "================================================================================\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1100: 1.810777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1200: 1.782127 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1300: 1.768001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1400: 1.771654 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1500: 1.756266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1600: 1.741917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1700: 1.727381 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.696020 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1900: 1.704270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2000: 1.686689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "x their reque clial his theses the commetes al lans colambes were systrard sove \n",
      "urenting in ance effect of al pase gainst of the kearches in arminulard estriubl\n",
      "hpaer and of the tords lacemed chance englisher tour commution of dod a year cas\n",
      "x fanthss hey ced to stanle fran all word life beine in mude bosed defenet yevo \n",
      "k to tukery sambs leviled and bodbs for in the pused to gove the fage hexcetherc\n",
      "================================================================================\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2100: 1.697955 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2200: 1.712594 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2300: 1.713615 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2400: 1.690422 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2500: 1.695602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2600: 1.674318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2700: 1.688214 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2800: 1.683385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2900: 1.681385 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 3000: 1.688623 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "ose have island securaber elemative flu war broakm igendif of changazay the seul\n",
      "to oded in is kase assantist to this cerromes conlay contain purn english some o\n",
      "zeac for ustlis of the few is of instaption on shelic of a cortus masophoundronc\n",
      "bation or cotubed of pophowo in the xiverale the braun three canre dat a gatepho\n",
      "phom of tran tenms of ze in deghishn in o the press druaderijit it in the k lead\n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3100: 1.657170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3200: 1.639032 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3300: 1.648367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3400: 1.636611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3500: 1.678782 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3600: 1.655941 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3700: 1.652153 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3800: 1.660404 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3900: 1.648703 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4000: 1.640645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "balleracian creuse be causer stribers a recondance of table plaiet of assiclail \n",
      "jan pell defined but in setsy and a this untimpers resulting national wordd that\n",
      "bers one one zero battthre openlio civs devighting are depertitues of lation for\n",
      "thrybeme powiterousen others in a veryset russier cyles requied from forne for s\n",
      "k the rate s the faunts but first art triegard of constomous privite marke wwal \n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4100: 1.619821 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4200: 1.616332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4300: 1.624637 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4400: 1.607441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4500: 1.643831 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4600: 1.625515 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4700: 1.621769 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4800: 1.607879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4900: 1.625670 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5000: 1.618358 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "jaling cotely lyrgored onecon one aince charge colleman poller repotoring electr\n",
      "d bring dud convenate one zhr playing fores feate thbe nine in the fime repualio\n",
      "jach at sued chnondions of the p off curressix heatire the releas the introduced\n",
      "quent american moremef the dinari irises fakew of the american economic to berna\n",
      "verted in seaple were diovides in experies the specqucia one nine six new simplo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5100: 1.591083 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5200: 1.598449 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5300: 1.595641 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5400: 1.588439 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5500: 1.590280 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5600: 1.564436 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5700: 1.582314 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5800: 1.597680 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5900: 1.583501 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.583256 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "ola help equalef cloudup grounds contibing orbersic anstrades working ethnch one\n",
      "zink are since of first a capiolal lows abu bhird in the were oct its may distar\n",
      "g wave signed of the to physumes of had wintine priprontul opendy farther after \n",
      "nown his mid ables to havadran crost compure by the original nort attending the \n",
      "king his elecoteed whill naided of name feithely were guils somie and fount this\n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6100: 1.577506 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6200: 1.592859 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6300: 1.587551 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6400: 1.575165 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6500: 1.558997 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6600: 1.597747 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6700: 1.569166 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6800: 1.576621 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6900: 1.572885 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 7000: 1.588777 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "ke and cholow distance some hessaulturs one nine giving al is thei and times fro\n",
      "th shiunhis mumily hib year as the sames and complety mechanials are excourting \n",
      "f hibse and childd and the italium e by quasins all saly see inclidential poturi\n",
      "ted to nations had indever xand guits holination accurediur yftoke an and was is\n",
      "s of vease andd with spacilation of lack an hav baspree mich move lives six five\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    # create feed \n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    # run one step \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], \n",
    "                                        feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' \n",
    "            % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' \n",
    "            % float(np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "      # Periodically generate some samples\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed     = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed       = sample(prediction)\n",
    "            sentence  += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "    \n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions   = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' \n",
    "            % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential second solution: Use a LARGE matrix that has ALL the vars, and use single batch_mul operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_NODES = num_nodes = 64\n",
    "VOCAB_SIZE = vocabulary_size\n",
    "BATCH_SIZE = batch_size\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "    \n",
    "  # I, F, C, O matrix  (note the extra dimension!)\n",
    "  ifco_x = tf.Variable(tf.truncated_normal([4, VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  ifco_m = tf.Variable(tf.truncated_normal([4, NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  ifco_b = tf.Variable(tf.zeros([4, 1, NUM_NODES]))\n",
    "\n",
    "  '''\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([VOCAB_SIZE, NUM_NODES], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([NUM_NODES,  NUM_NODES], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  \n",
    "  # combine all vars together (to reduce matrix multiplications)\n",
    "  px = tf.concat(1, [ix, fx, cx, ox])   # Input, Forget, (Memory) Update, Ouput\n",
    "  pm = tf.concat(1, [im, fm, cm, om])\n",
    "  pb = tf.concat(1, [ib, fb, cb, ob])\n",
    "  '''\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "  saved_state  = tf.Variable(tf.zeros([BATCH_SIZE, NUM_NODES]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([NUM_NODES, VOCAB_SIZE], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"\n",
    "    Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\n",
    "    Returns output (calculated) and current state\n",
    "    \"\"\"\n",
    "    # add input and output to extra dimension\n",
    "    in_list  = tf.pack([i, i, i, i])\n",
    "    out_list = tf.pack([o, o, o, o])\n",
    "    \n",
    "    # batch matrix mul\n",
    "    ins  = tf.batch_matmul(in_list, ifco_x)\n",
    "    outs = tf.batch_matmul(out_list, ifco_m)\n",
    "    # calc hidden \n",
    "    h_x  = ins + outs + ifco_b\n",
    "    \n",
    "    # input_gate  = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    input_gate    = tf.sigmoid(h_x[0, :, :])\n",
    "    \n",
    "    # forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    forget_gate   = tf.sigmoid(h_x[1, :, :])\n",
    "    \n",
    "    # update      = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    update        = tf.tanh(h_x[2, :, :])\n",
    "    \n",
    "    # state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    state         = forget_gate * state + input_gate * update\n",
    "    \n",
    "    # output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    output_gate   = tf.sigmoid(h_x[3, :, :])\n",
    "    h             = output_gate * tf.tanh(state)\n",
    "    return h, state     # output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, \n",
    "                                     shape=[BATCH_SIZE,VOCAB_SIZE]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output  = saved_output\n",
    "  state   = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss   = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, \n",
    "                                                                    tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, \n",
    "                                             global_step, \n",
    "                                             5000, \n",
    "                                             0.1, \n",
    "                                             staircase=True)\n",
    "  optimizer    = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)   # gradient clipping to prevent exploding\n",
    "  optimizer    = optimizer.apply_gradients(zip(gradients, v), \n",
    "                                        global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input        = tf.placeholder(tf.float32, shape=[1, VOCAB_SIZE])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  saved_sample_state  = tf.Variable(tf.zeros([1, NUM_NODES]))\n",
    "  reset_sample_state  = tf.group(saved_sample_output.assign(tf.zeros([1, NUM_NODES])),\n",
    "                                 saved_sample_state.assign (tf.zeros([1, NUM_NODES])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, \n",
    "                                          saved_sample_output, \n",
    "                                          saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294040 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "buoihfg  rop yas  d jndywee mew yr   e wp ejp mfkz t  cpodrga fl kc c jmn  f ob \n",
      "abm urhrosqo nr oqcio naki ni msird neax jsupukxtjvaathfglvktredub lpefztqi tmas\n",
      " ctapqrcst  rhst ue swa  s mrvam  vh lshoetavswt e qd hfi kkrotnru yjntn heuqbne\n",
      "eemirh hrsqfmpfrwtinndt s ohsylw  c osi lcsfrdenwuje yh jdcoycojrrtmnty s k i md\n",
      "moydrreue fuam  zqdpi osi cek dp vc s yefdmtcqczrkbjc jhftygn nhbtln agkjg a myr\n",
      "================================================================================\n",
      "Validation set perplexity: 19.83\n",
      "Average loss at step 100: 2.594563 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.29\n",
      "Validation set perplexity: 10.73\n",
      "Average loss at step 200: 2.246513 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.37\n",
      "Validation set perplexity: 8.81\n",
      "Average loss at step 300: 2.071486 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.98\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 400: 1.982391 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 500: 1.979789 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 600: 1.908371 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 700: 1.876620 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 800: 1.857106 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 900: 1.845911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1000: 1.780455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "================================================================================\n",
      "ch uninationend afop the leven of have lisk can resurted coser as heme in a reas\n",
      "d loce americal of prothesions in in or gis taed frovual doverol peoker five bas\n",
      "jost mapred as deajs sent exculityan effecting to smilance of ereng low in p y a\n",
      "h itiperarthmed resernation best otions in us parlin scamn uraaruactry comenase \n",
      "t of not is stank caller ismasel x pre or olland is anxy conseens of recrative e\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.753242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1200: 1.779600 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1300: 1.762147 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1400: 1.734157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1500: 1.721075 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1600: 1.709061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1700: 1.728702 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1800: 1.694045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1900: 1.700384 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2000: 1.708930 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "================================================================================\n",
      "y that toblesion of kamay of weetroin leme basacolicises jown wailmas ford spell\n",
      "vome god messedwed tempproon bacarrip in murklope abbritorated consorte in kast \n",
      "hor a dasited to in lowargetunition his was faming hown agrimater when reaily ot\n",
      "x assimm equitedow theorseriams his nentoly is flansfable been the feronsecated \n",
      "man other tenoghterwork the besces releese to jown chill bassouater mowned m com\n",
      "================================================================================\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2100: 1.688853 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2200: 1.664171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2300: 1.676713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2400: 1.674565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2500: 1.697453 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2600: 1.664030 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2700: 1.684512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2800: 1.639642 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2900: 1.648331 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3000: 1.652646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "x establimy use antipley ale users hat the compution offareration chooding kon t\n",
      "urcha wimiging of limes in refores the depiter consideren zerakern with varter s\n",
      "ates for so lings which halj new a poperued to are usid highsated was the long t\n",
      "x lodemens wisiooslish or aurused for abourdan tencrumes apolimin na marced the \n",
      "mer emplyshes after sorns are stedial leaders the secon theirs used states produ\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3100: 1.649643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3200: 1.638842 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3300: 1.624292 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3400: 1.629678 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3500: 1.619733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3600: 1.621368 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3700: 1.618901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3800: 1.612578 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3900: 1.606434 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4000: 1.609571 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "ited is is capruslica israelly as the motroup here iteeweigable devidentifie pla\n",
      "vespibe and terdy bird fiemering they isman s the two zero zero zero zero zero t\n",
      "mpergality of hdd plational of additism speakus officulan itsed on the one activ\n",
      "chel inding soum englionsforecal standed which blackly him quiccuptilas of canab\n",
      "mled leaming these unled argel factive mervigue indalilist not true a a reendase\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4100: 1.609214 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4200: 1.593588 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4300: 1.577197 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4400: 1.607138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4500: 1.620790 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4600: 1.617358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4700: 1.587575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4800: 1.568776 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4900: 1.584380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5000: 1.612713 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "pxcom weiranom only are teleg be cursion conquels was subinstaltary alsonsing of\n",
      "que and obtains of s demonne indivil five five two nine two albailund four six s\n",
      "x the weblecabline mane network classing he ters of torver to work an each the m\n",
      "mor times limin dul ruls statest carine finans it it also is reer as a exponsed \n",
      " i kas a grashing inforsh obyois american the mestene his history but mater the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5100: 1.622516 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5200: 1.614710 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5300: 1.577049 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5400: 1.580297 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5500: 1.568297 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5600: 1.597139 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5700: 1.556822 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.87\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5800: 1.563834 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5900: 1.580018 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6000: 1.546233 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "port whet ahoud beliea in and for michibord begually three three zero the two fi\n",
      "le as ghan two kamic taspeer unnow but jaalbu mart has seruroms then hamits edub\n",
      "ettic changents and a serves one name comely for abstruc this v unionified amoro\n",
      "velse this for alson the papar h manant shire the fameus for five zero zero gif \n",
      "xicuatalyses after ski ternmenua the warle the occuped thus are the states withh\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6100: 1.564892 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6200: 1.590433 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6300: 1.595917 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6400: 1.627442 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6500: 1.624405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6600: 1.590785 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6700: 1.575616 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6800: 1.564693 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6900: 1.552958 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 7000: 1.569057 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "hemendents was lives and p not smalled in arrowfoo were field dire in two three \n",
      "nicement and also places work moned the urrock supports to just hase exensity sh\n",
      "bery prubleat was aparonstance roy howfision stoxist in reason neess of linkay o\n",
      "folloticks atapthamed to his dejactives for ackudeven engitor which arreagal for\n",
      "zingoo jets with villance can promunitioning lion so metraudals other flan nalit\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    # create feed \n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    # run one step \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], \n",
    "                                        feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' \n",
    "            % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' \n",
    "            % float(np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "      # Periodically generate some samples\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed     = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed       = sample(prediction)\n",
    "            sentence  += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "    \n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions   = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' \n",
    "            % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See here: https://github.com/kcbighuge/tensorflow-deeplearning/blob/master/6_lstm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
